---
title: "HARVARD EXTENSION SCHOOL"
subtitle: "EXT CSCI E-106 Model Data Class Group Project Template"
author:
- Author One
- Author Two
- Author Three
- Author Four
- Author Five
- Author Six
- Author Seven
- Author Eight
- Author Nine

tags: [logistic, neuronal networks, etc..]
abstract: |
  This is the location for your abstract.

  It must consist of two paragraphs.
date: "`r format(Sys.time(), '%d %B %Y')`"
geometry: margin=1.3cm
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---
\newpage
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
Titanic.Data<-read.csv("Titanic_Survival_Data.csv")
```
\newpage
## Classify whether a passenger on board the maiden voyage of the RMS Titanic in 1912 survived given their age, sex and class. Sample-Data-Titanic-Survival.csv to be used in the Final Project


| Variable| Description |
| :-------:| :------- |
| pclass| **Passanger Class, could be 1st, 2nd or 3rd**    |
| survived| *Survival Status: 0=No, 1=Yes*    |
| name| *Name of the Passanger*    |
| Sex| *Sex*    |
| sibsp| *Number of Siblings or Spouses aboard*    |
| parch| *Number of Parents or Children aboard*    |
| ticket| *Ticket Number*    |
| fare| *Passenger Fare*    |
| cabin| *Cabin number, “C85” would mean the cabin is on deck C and is numbered 85.*    |
| embarked| *Port of Embarkation: C=Cherburg, S=Southampton, Q=Queenstown*    |
| boat| *Lifeboat ID, if passenger survived*    |
| body| *Body number (if passenger did not survive and body was recovered*    |
| home.dest| *The intended home destination of the passenger*    | 

\newpage
## Instructions:
0.  Join a team with your fellow students with appropriate size (Up to Nine Students total)
If you have not group by the end of the week of April 11 you may present the project by yourself or I will randomly assign other stranded student to your group. I will let know the final groups in April 11.
1.  Load and Review the dataset named "Titanic_Survival_Data.csv"
2.	Create the train data set which contains 70% of the data and use set.seed (15). The remaining 30% will be your test data set.
3.	Investigate the data and combine the level of categorical variables if needed and drop variables as needed. For example, you can drop id, Latitude, Longitude, etc.
4.	Build appropriate model to predict the probability of survival. 
5.	Create scatter plots and a correlation matrix for the train data set. Interpret the possible relationship between the response.
6.	Build the best models by using the appropriate selection method. Compare the performance of the best logistic linear models. 
7.	Make sure that model assumption(s) are checked for the final model. Apply remedy measures (transformation, etc.) that helps satisfy the assumptions. 
8.	Investigate unequal variances and multicollinearity. 
9.	Build an alternative to your model based on one of the following approaches as applicable to predict the probability of survival: logistic regression, classification Tree, NN, or SVM.  Check the applicable model assumptions. Explore using a negative binomial regression and a Poisson regression. 
10.	Use the test data set to assess the model performances from above.
11.	Based on the performances on both train and test data sets, determine your primary (champion) model and the other model which would be your benchmark model.
12.	Create a model development document that describes the model following this template, input the name of the authors, Harvard IDs, the name of the Group, all of your code and calculations, etc..:

## Due Date: May 12 2025 1159 pm hours EST

**Notes**
**No typographical errors, grammar mistakes, or misspelled words, use English language**
**All tables need to be numbered and describe their content in the body of the document**
**All figures/graphs need to be numbered and describe their content**
**All results must be accurate and clearly explained for a casual reviewer to fully understand their purpose and impact**
**Submit both the RMD markdown file and PDF with the sections with appropriate explanations. A more formal document in Word can be used in place of the pdf file but must include all appropriate explanations.**
```{r data loading and cleaning}
titanic <- read.csv('Titanic_Survival_Data.csv')
str(titanic)
titanic <- titanic[-nrow(titanic), ]

titanic <- titanic[,-c(3,8,12,13,14)]
titanic$sex <- ifelse(titanic$sex == "female", 0, 1)
titanic$embarkedC <- as.numeric(ifelse(titanic$embarked == "C", 1, 0))
titanic$embarkedS <- as.numeric(ifelse(titanic$embarked == "S", 1, 0))
titanic$cabin <- substring(titanic$cabin, 1, 1)
titanic$cabinA <- as.numeric(ifelse(titanic$cabin == "A", 1, 0))
titanic$cabinB <- as.numeric(ifelse(titanic$cabin == "B", 1, 0))
titanic$cabinC <- as.numeric(ifelse(titanic$cabin == "C", 1, 0))
titanic$cabinD <- as.numeric(ifelse(titanic$cabin == "D", 1, 0))
titanic$cabinE <- as.numeric(ifelse(titanic$cabin == "E", 1, 0))
titanic$cabinF <- as.numeric(ifelse(titanic$cabin == "F", 1, 0))
titanic$cabinG <- as.numeric(ifelse(titanic$cabin == "G", 1, 0))
titanic$cabinT <- as.numeric(ifelse(titanic$cabin == "T", 1, 0))

titanic$age[is.na(titanic$age)] <- median(titanic$age, na.rm = TRUE)
titanic$fare[is.na(titanic$fare)] <- median(titanic$fare, na.rm = TRUE)


titanic <- titanic[,-c(8,9)]

set.seed(15)
n <- nrow(titanic)
train_index <- sample(1:n, n*0.7)
train_titanic <- titanic[train_index, ]
test_titanic <- titanic[-train_index, ]

sapply(titanic, class)

# age, fare have blanks
# for cabin, pull only the first letter and use that for the model (all n/a put into the same category).
# Potentially do age w/ median and another model where we drop n/a ages.
```

```{r Catherine cleaning}
#install.packages("fastDummies")
library(ISLR2)
library(tidyverse)
library(caret)
library(MASS)
library(fastDummies)
library(stringr)
odata<- read.csv("Titanic_Survival_Data.csv")
cat("Size of entire data set:", nrow(odata), "\n")

#Remove Unneeded Columns'
data <- odata[, !(names(odata) %in% c("name", "ticket", "boat","body","home.dest"))]

#Replace NAs in age column with Median value 
median_age <- median(data$age, na.rm = TRUE)
data <- data %>%
  mutate(age = ifelse(is.na(age), median_age, age))

#Extract deck letter from cabin
data$deck <- substr(data$cabin, 1,1)
data$cabin <- NULL

#Create dummy variables
data <- data %>%
  fastDummies :: dummy_cols(select_columns = c("sex", "embarked","deck"),
                            remove_selected_columns = TRUE)

#Remove NAs
data <-na.omit(data)
#head(data)
set.seed(15)

#split data into train and test
datasplit <- data %>%
  group_by(survived) %>%
  group_split() %>%
  lapply(function(df) sample(seq_len(nrow(df)), size = 0.7 * nrow(df))) %>%
  unlist()

data_train<-data[datasplit,]
data_test<-data[-datasplit,]

#check the data
cat("Training data set size:", nrow(data_train), "\n")
cat("Testing data set size:", nrow(data_test), "\n")
cat("Size of entire data set:", nrow(data), "\n")
head(data)
```

```{r Group cleaning code}
library(ISLR2)
library(tidyverse)
library(caret)
library(MASS)
library(fastDummies)
library(stringr)
library(VIM)

odata <- read.csv("Titanic_Survival_Data.csv")
cat("Size of entire data set:", nrow(odata), "\n")

#Extract deck letter from cabin
odata$deck <- substr(odata$cabin, 1,1)

# Replace "" with NA in the 'deck' column
odata$deck[odata$deck == ""] <- NA

# Convert 'cabin' to factor
odata$deck <- as.factor(odata$deck)

# Apply kNN imputation just to Cabin column
odata <- kNN(odata, variable = "deck", k = 5)

# Check that NAs were imputed
sum(is.na(odata$deck))        # Original
#sum(is.na(data.clean.imputed$deck)) # After

data.clean = odata[, !(names(odata) %in% c("name", "ticket", "boat","body","home.dest"))]

# Remove cabin col:
data.clean$cabin <- NULL

#Replace NAs in age column with Median value 
median_age <- median(data.clean$age, na.rm = TRUE)
data.clean <- data.clean %>%
  mutate(age = ifelse(is.na(age), median_age, age))


# Dummifying pclass:
data.clean$pclass_1 = ifelse(data.clean$pclass == 1, 1, 0)
data.clean$pclass_2 = ifelse(data.clean$pclass == 2, 1, 0)

# Dummifying sex:
data.clean$sex_M = ifelse(data.clean$sex == 'male', 1, 0)

# Dummifying embarked:
data.clean$embarked_C = ifelse(data.clean$embarked == 'C', 1, 0)
data.clean$embarked_Q = ifelse(data.clean$embarked == 'Q', 1, 0)

# Dummifying deck:
data.clean$deck_A = ifelse(data.clean$deck == 'A', 1, 0)
data.clean$deck_B = ifelse(data.clean$deck == 'B', 1, 0)
data.clean$deck_C = ifelse(data.clean$deck == 'C', 1, 0)
data.clean$deck_D = ifelse(data.clean$deck == 'D', 1, 0)
data.clean$deck_E = ifelse(data.clean$deck == 'E', 1, 0)
data.clean$deck_F = ifelse(data.clean$deck == 'F', 1, 0)
data.clean$deck_G = ifelse(data.clean$deck == 'G', 1, 0)

# Dummifying sibsp:
data.clean$sibsp_1 = ifelse(data.clean$sibsp == 1, 1, 0)
data.clean$sibsp_2 = ifelse(data.clean$sibsp == 2, 1, 0)
data.clean$sibsp_3 = ifelse(data.clean$sibsp == 3, 1, 0)
data.clean$sibsp_4 = ifelse(data.clean$sibsp == 4, 1, 0)
data.clean$sibsp_5 = ifelse(data.clean$sibsp == 5, 1, 0)
data.clean$sibsp_8 = ifelse(data.clean$sibsp == 8, 1, 0)

# Dummifying sibsp to 2 categories:
#data.clean$sibsp_y = ifelse(data.clean$sibsp > 0, 1, 0)

# Dummifying parch:
data.clean$parch_1 = ifelse(data.clean$parch == 1, 1, 0)
data.clean$parch_2 = ifelse(data.clean$parch == 2, 1, 0)
data.clean$parch_3 = ifelse(data.clean$parch == 3, 1, 0)
data.clean$parch_4 = ifelse(data.clean$parch == 4, 1, 0)
data.clean$parch_5 = ifelse(data.clean$parch == 5, 1, 0)
data.clean$parch_6 = ifelse(data.clean$parch == 6, 1, 0)
data.clean$parch_9 = ifelse(data.clean$parch == 9, 1, 0)

# Dummifying parch to 2 categories:
#data.clean$parch_y = ifelse(data.clean$parch > 0, 1, 0)

# Removing Dummified cols:
data.clean = subset(data.clean, select  = -c(pclass, sex, embarked, deck, deck_imp))#, sibsp, parch))

data.clean = na.omit(data.clean)

cat(nrow(odata) - nrow(data.clean),'rows were removed from original dataset')

set.seed(567)
train_indices = sample(1 : nrow(data.clean), size = 0.7005*nrow(data.clean), replace = FALSE)
train = data.clean[train_indices,]
test = data.clean[-train_indices,]
cat("We are using:", nrow(train)/nrow(data.clean) * 100, '% of the data for training')

```

```{r Group model creation code - sibsp and parch as individual categories}
# Expected model structure: survived ~ pclass + Sex + sibsp + parch + ticket + fare + cabin + embarked

# Run the code to clean the data as normal
simple_model1 <- lm(survived ~ pclass_1, data = train)
simple_model2 <- lm(survived ~ sex_M, data = train)
simple_model3 <- lm(survived ~ age, data = train)
mulvar_model <- lm(survived ~ ., data = train)
summary(mulvar_model)

lmod <- glm(as.factor(survived) ~ ., family = binomial, data = train)

summary(lmod)

y_hat_mulvar_train<-predict(mulvar_model, data = train)
predictions_train <- ifelse(y_hat_mulvar_train > 0.5, 1, 0)
ModelTrain_mulvar<-data.frame(obs = train$survived, pred=predictions_train)
defaultSummary(ModelTrain_mulvar)

y_hat_mulvar_test<-predict(mulvar_model, newdata = test)
predictions_test <- ifelse(y_hat_mulvar_test > 0.5, 1, 0)
ModelTest_mulvar<-data.frame(obs = test$survived, pred=predictions_test)
defaultSummary(ModelTest_mulvar)


y_hat_log_train<-predict(lmod, data = train)
ModelTrain_lmod<-data.frame(obs = train$survived, pred=y_hat_log_train)
defaultSummary(ModelTrain_mulvar)

y_hat_log_test<-predict(lmod, newdata = test)
ModelTest_lmod<-data.frame(obs = test$survived, pred=y_hat_log_test)
defaultSummary(ModelTest_lmod)

```

```{r Group model creation code - sibsp and parch as 2 categories}
# Expected model structure: survived ~ pclass + Sex + sibsp + parch + ticket + fare + cabin + embarked

# Run the code to clean all of the data up to the sibsp and parch cleaning. For that section, only run the code to create 1 dummy column.
simple_model1 <- lm(survived ~ pclass_1, data = train)
simple_model2 <- lm(survived ~ sex_M, data = train)
simple_model3 <- lm(survived ~ age, data = train)
sib_model <- lm(survived ~ sibsp_y, data = train)
summary(sib_model)

mulvar_model <- lm(survived ~ ., data = train)
summary(mulvar_model)

lmod <- glm(as.factor(survived) ~ ., family = binomial, data = train)

summary(lmod)

#y_hat_sib_train<-predict(sib_model, data = train)
#predictions_train <- ifelse(y_hat_sib_train > 0.5, 1, 0)
#confusion_matrix_train <- table(Actual = train$survived, Predicted = predictions_train)
#print(confusion_matrix_train)

#y_hat_sib_test<-predict(sib_model, newdata = test)
#predictions_test <- ifelse(y_hat_sib_test > 0.5, 1, 0)
#confusion_matrix_test <- table(Actual = test$survived, Predicted = predictions_test)
#print(confusion_matrix_test)

y_hat_mulvar_train<-predict(mulvar_model, data = train)
predictions_train <- ifelse(y_hat_mulvar_train > 0.5, 1, 0)
ModelTrain_mulvar<-data.frame(obs = train$survived, pred=predictions_train)
defaultSummary(ModelTrain_mulvar)

y_hat_mulvar_test<-predict(mulvar_model, newdata = test)
predictions_test <- ifelse(y_hat_mulvar_test > 0.5, 1, 0)
ModelTest_mulvar<-data.frame(obs = test$survived, pred=predictions_test)
defaultSummary(ModelTest_mulvar)


y_hat_log_train<-predict(lmod, data = train)
ModelTrain_lmod<-data.frame(obs = train$survived, pred=y_hat_log_train)
defaultSummary(ModelTrain_mulvar)

y_hat_log_test<-predict(lmod, newdata = test)
ModelTest_lmod<-data.frame(obs = test$survived, pred=y_hat_log_test)
defaultSummary(ModelTest_lmod)

```

```{r Group model creation code - sibsp and parch dummies as continuous}
# Expected model structure: survived ~ pclass + Sex + sibsp + parch + ticket + fare + cabin + embarked

# Run the code to clean the data, but stop before you transform the sibsp and parch into categorical data.
simple_model1 <- lm(survived ~ pclass_1, data = train)
simple_model2 <- lm(survived ~ sex_M, data = train)
simple_model3 <- lm(survived ~ age, data = train)
mulvar_model <- lm(survived ~ ., data = train)
summary(mulvar_model)

lmod <- glm(as.factor(survived) ~ ., family = binomial, data = train)

summary(lmod)

y_hat_mulvar_train<-predict(mulvar_model, data = train)
predictions_train <- ifelse(y_hat_mulvar_train > 0.5, 1, 0)
ModelTrain_mulvar<-data.frame(obs = train$survived, pred=predictions_train)
defaultSummary(ModelTrain_mulvar)

y_hat_mulvar_test<-predict(mulvar_model, newdata = test)
predictions_test <- ifelse(y_hat_mulvar_test > 0.5, 1, 0)
ModelTest_mulvar<-data.frame(obs = test$survived, pred=predictions_test)
defaultSummary(ModelTest_mulvar)


y_hat_log_train<-predict(lmod, data = train)
ModelTrain_lmod<-data.frame(obs = train$survived, pred=y_hat_log_train)
defaultSummary(ModelTrain_mulvar)

y_hat_log_test<-predict(lmod, newdata = test)
ModelTest_lmod<-data.frame(obs = test$survived, pred=y_hat_log_test)
defaultSummary(ModelTest_lmod)

```

```{r Derek original model creation code}
# Expected model structure: survived ~ pclass + Sex + sibsp + parch + ticket + fare + cabin + embarked

simple_model1 <- lm(survived ~ pclass, data = train_titanic)
simple_model2 <- lm(survived ~ sex, data = train_titanic)
simple_model3 <- lm(survived ~ age, data = train_titanic)
mulvar_model <- lm(survived ~ ., data = train_titanic)
summary(mulvar_model)

lmod <- glm(as.factor(survived) ~ ., family = binomial, data = train_titanic)

summary(lmod)

y_hat_mulvar_train<-predict(mulvar_model, data = train_titanic)
predictions_train <- ifelse(y_hat_mulvar_train > 0.5, 1, 0)
ModelTrain_mulvar<-data.frame(obs = train_titanic$survived, pred=predictions_train)
defaultSummary(ModelTrain_mulvar)

y_hat_mulvar_test<-predict(mulvar_model, newdata = test_titanic)
predictions_test <- ifelse(y_hat_mulvar_test > 0.5, 1, 0)
ModelTest_mulvar<-data.frame(obs = test_titanic$survived, pred=predictions_test)
defaultSummary(ModelTest_mulvar)


y_hat_log_train<-predict(lmod, data = train_titanic)
ModelTrain_lmod<-data.frame(obs = train_titanic$survived, pred=y_hat_log_train)
defaultSummary(ModelTrain_lmod)

y_hat_log_test<-predict(lmod, newdata = test_titanic)
ModelTest_lmod<-data.frame(obs = test_titanic$survived, pred=y_hat_log_test)
defaultSummary(ModelTest_lmod)

```

```{r plots and correlation of model}
library(ggplot2)
par(mfrow=c(2,2))
plot(mulvar_model)
# Plots show that a linear model is not appropriate for this data.

par(mfrow=c(2,2))
plot(lmod)
# Plots show that a linear model is not appropriate for this data.

ggplot(train_titanic, aes(survived)) +
  geom_bar()

ggplot(train_titanic, aes(pclass, survived)) +
  geom_point()


survival_by_sex <- train_titanic %>%
  group_by(sex) %>%
  summarize(survived_count = sum(survived))

ggplot(survival_by_sex, aes(x= sex, y = survived_count, fill = factor(sex))) +
  geom_bar(stat = 'identity') + 
  scale_x_discrete(labels = c("0" = "Female", "1" = "Male")) +
  scale_fill_discrete(labels = c("0" = "Female", "1" = "Male"), name = "Sex") +
  geom_text(aes(label = survived_count), vjust = -0.5) +
  labs(
    title = "Number of People Who Survived by Sex",
    x = "Sex",
    y = "Survived",
    fill = "Sex"
  ) +
  theme_minimal()

ggplot(train_titanic, aes(age, survived)) +
  geom_point()


cor(train_titanic)
pairs(train_titanic[c(1:4,7,13)])
# Since this data is mainly categorical, the scatterplot and correlation matrix are not very useful.

```


```{r model and graphs based on group cleaned data}
simple_model1 <- lm(survived ~ pclass, data = train_titanic)
simple_model2 <- lm(survived ~ sex, data = train_titanic)
simple_model3 <- lm(survived ~ age, data = train_titanic)
mulvar_model <- lm(survived ~ ., data = train_titanic)
lmod <- glm(as.factor(survived) ~ ., family = binomial, data = train_titanic)

summary(lmod)


par(mfrow=c(2,2))
plot(mulvar_model)
# Plots show that a linear model is not appropriate for this data.

cor(train_titanic)
pairs(train_titanic[c(1:4,7,13)])
# Since this data is mainly categorical, the scatterplot and correlation matrix are not very useful.
```
Data Cleaning and Visualization
- Check for N/A and missing values
- Check correlations among the data
- Look for outliers
- Graphs: Box & Whisker, Scatterplot, Bar/Histogram


Different Models to Create
- Simple Linear Regression
- Multivariate Regression
- Ridge Regression
- Lasso Regression
- Elastic Net
- Robust Regression (Huber and Bisquare)
- Regression Tree
- Classification Tree


Model Review
- Brown-Forsythe and Breusch-Pagan
- Shapiro-Wilks
- Large Leverage test
- Outlier test
- Influential point test (cook)
- Multicollinearity test (vif)
- BoxCox
- Stepwise 

***Write Up Draft Hao***

* Bown-Forsythe Test Results:
(Usually apply to linear only)
Both Linear and Logistic Regression Model suggests no significant heteroskedasticity.

Linear Model: F-Value 0.0558, p-value = 0.8133. Fail to to reject the null Hypothesis. No evidence of unequal variances. 
Logistic Model: F-Value 0.0031, p-value = 0.9553. Fail to to reject the null Hypothesis. No evidence of unequal variances. 

* Breusch Pagan Test:
Both Linear and logistic Regression Model suggests no evidence of heteroskedasticity.

Linear Model: Chi^2 3.31, p-value 0.069. Fail to reject Null hypothesis.
Logistic Model: Chi^2 0.60, p-value 0.440. Fail to reject Null Hypothesis.


* Shapiro-Wilk Test:
Linear Model: W = 0.96, p-value <0.05. Reject H0, Residuals are not normal.

* Large Leverage and Outlier Test
Linear Model: Observation 92 was flagged as both an outlier and high-leverage point. 
Logistic Model: Found more high-leverage points, but no single observations both appear as leverage and outlier.

* Influential Point Test
Many of the same observations were influential in both linear and logistic models.

* Multicollinearity Test:
The Variance Inflation Factors (VIF) for all predictors are around 1-2, below 5, indicating low levels of correlation.

* BOX Cox transformation:

Although the Shapiro-Wilk test indicated that the residuals from the linear model deviate from normality, there was no evidence of heteroskedasticity from either the Brown-Forsythe or Breusch-Pagan tests. In addition, multicollinearity levels were low, and influence diagnostics were carefully reviewed. As a result, no Box-Cox transformation was applied, as the assumptions of linear regression were reasonably satisfied, and the transformation would not add meaningful value in this context.

* Stepwise:

Both original model and stepwise models perform similarly on train and test data, no significant overfiting problem. Overall, logistic step model appears to be reasonable stable, and is preferred in classifying survival, despite higher errors.

Linear Step Model:
Y = 1.16 - 0.15pclass - 0.52sex - 0.0045age - 0.032sibsp + 0.12embarkedC + 0.16cabinE + 0.21cabinF"
Logistic Step ModeL:
Y = 4.13 - 0.99pclass - 2.8sex - 0.031age - 0.25sibsp + 0.79embarkedC + 1.1cabinE + 1.3cabinF


| Model Type   | Version     | RMSE     | R-squared | MAE      |
|--------------|-------------|----------|-----------|----------|
| **Linear**   | Train       | 0.4482   | 0.3191    | 0.2009   |
|              | Test        | 0.4759   | 0.2808    | 0.2265   |
|              | Stepwise    | 0.4451   | 0.3163    | 0.1985   |
|              | Step Test   | 0.4772   | 0.2776    | 0.2275   |
| **Logistic** | Train       | 1.9644   | 0.3817    | 1.6320   |
|              | Test        | 2.0007   | 0.3373    | 1.7287   |
|              | Stepwise    | 1.9589   | 0.3790    | 1.6280   |
|              | Step Test   | 1.9957   | 0.3352    | 1.7220   |


```{r Hao Code Model Review}

# Brown-Forsythe Test on Linear Model
library(car)
#Create the group variable based on fitted values.Smaller than 655 (half of training data size)- group1, else group2
group <- ifelse(rank(fitted(mulvar_model)) <= 655, 1, 2)
ei <- residuals(mulvar_model)
leveneTest(ei, group, center=median)

# Brown-Forsythe Test on Linear Model
group <- ifelse(rank(fitted(lmod)) <= 655, 1, 2)
ei <- residuals(lmod)
leveneTest(ei, group, center=median)

# Breusch Pagan Test on Linear Model
ols_test_breusch_pagan(mulvar_model)
# Breusch Pagan Test on Logistic Model
ols_test_breusch_pagan(lmod)

# Normality Test -- Performed in plot section after model.

# Shapiro-Wilk test
shapiro.test(residuals(mulvar_model))

# Large Leverage Test and Outlier test
###Studentized residuals vs leverage plot
ols_plot_resid_lev(mulvar_model)
ols_plot_resid_lev(lmod)

# Influential Point Test
### Cooks distance plot
ols_plot_cooksd_bar(mulvar_model)
ols_plot_cooksd_bar(lmod)


# Run VIF on the logistic model
cat("VIF Results for Logistic Regression Model \n")
ols_vif_tol(lmod)

```
```{r Hao Code step model result and performance }
#step model on both regression model
mulvar_model_step <- step(mulvar_model, direction = "both")
lmod_step <- step(lmod, direction = "both")

summary(mulvar_model_step)
summary(lmod_step)

y_hat_mulvar_train<-predict(mulvar_model_step, data = train_titanic)
predictions_train <- ifelse(y_hat_mulvar_train > 0.5, 1, 0)
ModelTrain_mulvar_step<-data.frame(obs = train_titanic$survived, pred=predictions_train)
defaultSummary(ModelTrain_mulvar_step)

y_hat_mulvar_test<-predict(mulvar_model_step, newdata = test_titanic)
predictions_test <- ifelse(y_hat_mulvar_test > 0.5, 1, 0)
ModelTest_mulvar_step<-data.frame(obs = test_titanic$survived, pred=predictions_test)
defaultSummary(ModelTest_mulvar_step)


y_hat_log_train<-predict(lmod_step, data = train_titanic)
ModelTrain_lmod_step<-data.frame(obs = train_titanic$survived, pred=y_hat_log_train)
defaultSummary(ModelTrain_lmod_step)

y_hat_log_test<-predict(lmod_step, newdata = test_titanic)
ModelTest_lmod_step<-data.frame(obs = test_titanic$survived, pred=y_hat_log_test)
defaultSummary(ModelTest_lmod_step)

```





Graphs for Final Model
- Errors vs Fitted Values
- QQ Residuals
- Scale Location
- Residuals vs Leverage
- Scatterplot Matrix
- Component + Residual Plots (crPlots)
- Ridge plot (if best model)
- Lasso plot (both) (if best model)
- Cook's D Bar Plot
- Deleted Studentized Residual vs Predicted Values (ols_plot_resid_stud_fit(model))
- Outlier and Leverage Diagnostics (ols_plot_resid_lev(model))
- Tree vs Residual error (plotcp(model))
- Tree (rpart.plot(model, digits = 3))



Executive Summary

This section will describe the model usage, your conclusions and any regulatory and internal requirements. In a real world scenario, this section is for senior management who do not need to know the details. They need to know high level (the purpose of the model, limitations of the model and any issues).


\newpage
## I. Introduction (5 points)

*This section needs to introduce the reader to the problem to be resolved, the purpose, and the scope of the statistical testing applied. What you are doing with your prediction? What is the purpose of the model? What methods were trained on the data, how large is the test sample, and how did you build the model?*
The Titanic was a British-registered ship that set sail on its maiden voyage on April 10th, 1912 with 2,240 passengers and crew on board. On April 15th, 1912, the ship struck an iceberg, split in half, and sank to the bottom of the ocean (REF 1). In this report, we are going to analyze the data in the Titanic.csv file and use it to determine the best model for predicting whether someone on board would live or die. By creating this model, we hope to understand what factors a passenger could have taken into account in order to reduce their risk of death during the trip. We cleaned the data and split into into a train/test split in order to properly train our models. We created simple linear models, multivariate linear models, logistic models (both binomial and poisson), a regression tree, and a neural network model. The train sample size was 916 data points (70.03058%) and the test sample size was 392 data points (29.96942%). We built the models after examining the data and determining which predictor variables we thought would be most relevant for survival rate. Once we had our variables and training data, we created the models and examined the performance of the models on both training and testing data to determine if they were robust. We also examined if the model assumptions appeared to hold for each model.

\newpage
## I. Description of the data and quality (15 points)

*Here you need to review your data, the statistical test applied to understand the predictors and the response and how are they correlated. Extensive graph analysis is recommended. Is the data continuous, or categorical, do any transformation needed? Do you need dummies? *


\newpage
## III. Model Development Process (15 points)

*Build an appropriate model to predict probability of survival.  And of course,  create the train data set which contains 70% of the data and use set.seed (1023). The remaining 30% will be your test data set. Investigate the data and combine the level of categorical variables if needed and drop variables. For example, you can passenger name, cabin, etc.. *

\newpage
## IV. Model Performance Testing (15 points)

*Use the test data set to assess the model performances. Here, build the best model by using appropriate selection method. You may compare the performance of the best two logistic or other classification model selected. Apply remedy measures as applicable (transformation, etc.) that helps satisfy the assumptions of your particular model. Deeply investigate unequal variances and multicollinearity if warranted.  *


\newpage
## V. Challenger Models (15 points)

*Build an alternative model based on one of the following approaches to predict survival as applicable:logistic regression, decision tree, NN, or SVM, Poisson regression or negative binomial. Check the applicable model assumptions. Apply in-sample and out-of-sample testing, back testing and review the comparative goodness of fit of the candidate models. Describe step by step your procedure to get to the best model and why you believe it is fit for purpose.*

\newpage
## VI. Model Limitation and Assumptions (15 points)

*Based on the performances on both train and test data sets, determine your primary (champion) model and the other model which would be your benchmark model. Validate your models using the test sample. Do the residuals look normal? Does it matter given your technique? How is the prediction performance using Pseudo R^2, SSE, RMSE?  Benchmark the model against alternatives. How good is the relative fit? Are there any serious violations of the logistic model assumptions? Has the model had issues or limitations that the user must know? (Which assumptions are needed to support the Champion model?)* 


## VI. Model Limitation and Assumptions (15 points)

*Based on the performances on both train and test data sets, determine your primary (champion) model and the other model which would be your benchmark model. Validate your models using the test sample. Do the residuals look normal? Does it matter given your technique? How is the prediction performance using Pseudo R^2, SSE, RMSE?  Benchmark the model against alternatives. How good is the relative fit? Are there any serious violations of the logistic model assumptions? Has the model had issues or limitations that the user must know? (Which assumptions are needed to support the Champion model?)* 

The champion model selected is the filtered logistic regression model due to its superior test accuracy (84.44%) and F1 score (0.8778). The filtered logistic regression model was developed through backward elimination procedure, applied to the full base model. This process iteratively removed predictors with high p-values until only statistically significant predictors remained, based on threshold (p <0.05). The base model is used as the benchmark due to its similar performance and broader feature set.

"Y = 5.56 - 0.055age - 0.23sibsp + 1pclass_1 + 1.9pclass_2 - 3.2sex_M + 0.84embarked_C + 0.86embarked_Q - 2.5deck_A - 1.9deck_B - 2.1deck_C - 3.1deck_D - 4.6deck_F"

To further evaluate and benchmark the logistic regression models, we computed RMSE, R², and MAE using the predicted probabilities against the actual binary outcomes. While traditional R² is not appropriate for logistic models, these metrics provide insight into how well the predicted probabilities align with observed outcomes. The Filtered model appearss to have higher R² (0.4425>0.4342), lower RMSE(0.3945 < 0.3977) and MAE (0.1556 <0.1582). Additionally, model validation was done through train/test split with consistent performance across sets. The performance on test data and train data show minimal difference, indicating the model is robust, no overfitting problem.

To evaluate the stability and fit of the base and filtered logistic regression models, we conducted residual-based tests on their linear analogs. While logistic regression does not formally require normally distributed or homoscedastic residuals, these tests offer insights into model specification quality and residual behavior. 

Breusch-Pagan Test: Both models exhibit statistically significant heteroskedasticity (p < 0.01)

Shapiro-Wilk Test: Both models show highly significant deviations from normality (p < 0.01)

Both models show similar residual behavior, with slight heteroskedasticity and non-normality. These findings do not violate logistic regression assumptions but suggest the model could be further improved through additional transformations or interaction terms.

Multicollinearity was significantly lower in the filtered model (max VIF: 3.69 vs. 16.63), supporting better generalization and interpretability. The VIF is less than 5, confirmed no severe multicollinearity.


```{r Hao Code Model Review}

library(car)

# Normality Test
# Breusch Pagan Test on Logistic Model
BPtest_basemodel <- ols_test_breusch_pagan(lmod)
BPtest_filtered.model <- ols_test_breusch_pagan(binary.model.filtered)

# Shapiro-Wilk test
SWtest_basemodel <- shapiro.test(residuals(lmod))
SWtest_filteredmodel <- shapiro.test(residuals(binary.model.filtered))


# Combine into unified rows
diagnostics_summary <- data.frame(
  Model = c("Base.model", "Filtered.model"),
  BP_Statistic = c(BPtest_basemodel$bp, BPtest_filtered.model$bp),
  BP_pvalue = c(BPtest_basemodel$p, BPtest_filtered.model$p),
  Shapiro_W = c(SWtest_basemodel$statistic, SWtest_filteredmodel$statistic),
  Shapiro_pvalue = c(SWtest_basemodel$p.value, SWtest_filteredmodel$p.value)
)
diagnostics_summary

# Run VIF on the logistic model
cat("VIF Results for Base Logistic Regression Model \n")
ols_vif_tol(lmod)
cat("VIF Results for Filtered Logistic Regression Model \n")
ols_vif_tol(binary.model.filtered)

```

```{r}
# Base Model - Train
observations.train <- train$survived
y_hat_base_train <- predict(lmod, train, type = "response")
pred_base_train <- ifelse(y_hat_base_train > optimal.cutoff[1], 1, 0)
ModelTrain_base <- data.frame(obs = observations.train, pred = pred_base_train)
log.train.base <- defaultSummary(ModelTrain_base)

# Base Model - Test
observations.test <- test$survived
y_hat_base_test <- predict(lmod, test, type = "response")
pred_base_test <- ifelse(y_hat_base_test > optimal.cutoff[1], 1, 0)
ModelTest_base <- data.frame(obs = observations.test, pred = pred_base_test)
log.test.base <- defaultSummary(ModelTest_base)

# Base Model - Train
observations.train <- train$survived
y_hat_base_train <- predict(binary.model.filtered, train, type = "response")
pred_base_train <- ifelse(y_hat_base_train > optimal.cutoff[1], 1, 0)
ModelTrain_base <- data.frame(obs = observations.train, pred = pred_base_train)
filtered.log.train.base <- defaultSummary(ModelTrain_base)

# Base Model - Test
observations.test <- test$survived
y_hat_base_test <- predict(binary.model.filtered, test, type = "response")
pred_base_test <- ifelse(y_hat_base_test > optimal.cutoff[1], 1, 0)
ModelTest_base <- data.frame(obs = observations.test, pred = pred_base_test)
filtered.test.base <- defaultSummary(ModelTest_base)


data.frame(rbind(log.train.base,log.test.base,filtered.log.train.base,filtered.test.base))
  
```


\newpage
## VII. Ongoing Model Monitoring Plan (5 points)

*How would you picture the model needing to be monitored, which quantitative thresholds and triggers would you set to decide when the model needs to be replaced? What are the assumptions that the model must comply with for its continuous use?*
In order to maintain the effectiveness of the model, we would need to continue to test it on new data. Since the Titanic was a rare event, we do not have a lot of new data to test on the model, but we can still be prepared in case new data were to become available. The first step in monitoring the model is to determine specific thresholds that we expect the model to stay above. We would want the model to maintain certain R^2, RMSE, and MAE values in order to determine that the model is working correctly. One of the biggest concerns with our model is data drift. Since the Titanic sank over 100 years ago, the data that we are using from the model may not align with today relevant to ship travel today. 


\newpage
## VIII. Conclusion (5 points)

*Summarize your results here. What is the best model for the data and why?*

## Bibliography (7 points)

*Please include all references, articles and papers in this section.*
(REF 1) "https://www.noaa.gov/office-of-general-counsel/gc-international-section/rms-titanic-history-and-significance#:~:text=Titanic%2C%20launched%20on%20May%2031,than%201%2C500%20passengers%20and%20crew."

(REF 2) "https://prepineer.com/s/binomial-distribution/#:~:text=It%20gives%20us%20the%20probability%20of%20getting%20r%20events%20in,an%20infinite%20number%20of%20trials."

(REF 3) "https://www.evidentlyai.com/ml-in-production/model-monitoring"

## Appendix (3 points)

*Please add any additional supporting graphs, plots and data analysis.*


