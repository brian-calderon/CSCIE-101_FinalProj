<div>

---
title: "HARVARD EXTENSION SCHOOL"

subtitle: "EXT CSCI E-106 Model Data Class Group Project Template"

author:
  - "Author: Dinesh Bedathuru"
  - "Author: Brian Calderon"
  - "Author: Jeisson Hernandez"
  - "Author: Hao Fu"
  - "Author: Derek Rush"
  - "Author: Jeremy Tajonera"
  - "Author: Catherine Tully"
  
date: "`r format(Sys.time(), '%d %B %Y')`"

format:
  pdf: # Output in pdf
    toc: true # Include table of contents
    number-sections: true # Number section headings
    toc-depth: 2 # Include up to H2 in TOC
    keep-tex: true
    geometry: margin=1.3cm
    df-print: kable
    include-in-header: justify.tex # Justify all text
    lof: true         # ← List of Figures
    lot: true         # ← List of Tables
    
tags: [logistic, neuronal networks, titanic]

abstract: |
  In this project, our aim is to classify the probability of a passenger surviving the Titanic crash of 1912. We used a variety of linear and non-linear models to deduce the most accurate model and provide long-term stability in our predictions.
  
editor_options:
  markdown:
    wrap: 72
    
editor: 
  markdown: 
    wrap: 72
    
bibliography: references.bib # references file

csl: harvard-educational-review.csl # csl file for harvard style referencing
---

</div>

```{r setup, include = FALSE}
# Library loads
library(ISLR2)
library(tidyverse)
library(caret)
library(MASS)
library(fastDummies)
library(stringr)
library(ggplot2)
################################################################################
#                  Function to plot missing data in DF                         #
################################################################################

plot_missing_barchart <- function(df) {
  # Calculate % of NA or empty strings per column
  na_empty_pct <- sapply(df, function(col) {
    mean(is.na(col) | col == "")
  }) * 100
  
  # Create a dataframe from the result
  na_df <- data.frame(
    Column = names(na_empty_pct),
    PercentMissing = na_empty_pct
  )
  
  # Plot the bar chart
  p =  ggplot(na_df, aes(x = reorder(Column, -PercentMissing), 
                           y = PercentMissing)) +
          geom_bar(stat = "identity", fill = "steelblue") +
          coord_flip() +
          labs(title = "Percentage of NA or Empty Cells per Column",
          x = "Column",
          y = "Percentage (%)") +
          theme_minimal()
  return(p)
}
```

::: pagebreak
:::

Classify whether a passenger on board the maiden voyage of the RMS
Titanic in 1912 survived given their age, sex and class.
Sample-Data-Titanic-Survival.csv to be used in the Final Project

| Variable | Description |
|------------------------------------|------------------------------------|
| pclass | **Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)** |
| survived | **Survival (0 = No; 1 = Yes)** |
| name | **Name** |
| sex | **Sex** |
| age | **Age** |
| sibsp | **\# of siblings / spouses aboard the Titanic** |
| parch | **\# of parents / children aboard the Titanic** |
| ticket | **Ticket number** |
| fare | **Passenger fare** |
| cabin | **Cabin number** |
| embarked | **Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)** |
| boat | **Lifeboat ID, if passenger survived** |
| body | **Body number (if passenger did not survive and body was recovered** |
| home.dest | **The intended home destination of the passenger** |

::: pagebreak
:::

# Instructions:

0\. Join a team with your fellow students with appropriate size (Up to
Nine Students total) If you have not group by the end of the week of
April 11 you may present the project by yourself or I will randomly
assign other stranded student to your group. I will let know the final
groups in April 11.

1\. Load and Review the dataset named “Titanic_Survival_Data.csv” 2.
Create the train data set which contains 70% of the data and use
set.seed (15). The remaining 30% will be your test data set.

3\. Investigate the data and combine the level of categorical variables
if needed and drop variables as needed. For example, you can drop id,
Latitude, Longitude, etc.

4\. Build appropriate model to predict the probability of survival.

5\. Create scatter plots and a correlation matrix for the train data
set. Interpret the possible relationship between the response.

6\. Build the best models by using the appropriate selection method.
Compare the performance of the best logistic linear models.

7\. Make sure that model assumption(s) are checked for the final model.
Apply remedy measures (transformation, etc.) that helps satisfy the
assumptions.

8\. Investigate unequal variances and multicollinearity.

9\. Build an alternative to your model based on one of the following
approaches as applicable to predict the probability of survival:
logistic regression, classification Tree, NN, or SVM. Check the
applicable model assumptions. Explore using a negative binomial
regression and a Poisson regression.

10\. Use the test data set to assess the model performances from above.

11\. Based on the performances on both train and test data sets,
determine your primary (champion) model and the other model which would
be your benchmark model.

12\. Create a model development document that describes the model
following this template, input the name of the authors, Harvard IDs, the
name of the Group, all of your code and calculations, etc..

**Due Date: May 12 2025 1159 pm hours EST Notes No typographical errors,
grammar mistakes, or misspelled words, use English language All tables
need to be numbered and describe their content in the body of the
document All figures/graphs need to be numbered and describe their
content All results must be accurate and clearly explained for a casual
reviewer to fully understand their purpose and impact Submit both the
RMD markdown file and PDF with the sections with appropriate
explanations. A more formal.**

::: pagebreak
:::

# Executive Summary

We have tested five different models of which four were logistics
regression based and one was a poisson model. The best model with
regards to predictive accuracy was a "filtered" logistic regression
(filtered because we removed insignificant predictors) with an accuracy
of 84.18%.

The final model shows that the most significant predictors of survival
are: age, sex, and deck_F. \
Below we show the final calculated function:

```         
survived ~ 5.74 - 0.05 age - 0.025 sibsp + 0.80 pclass_1 + 1.82 pclass_2 - 3.27 sex_M + 0.83 embarked_C + 0.78 embarked_Q - 2.55 deck_A - 1.89 deck_B - 2.02 deck_C - 3.00 deck_D - 4.70 deck_F
```

::: pagebreak
:::

# Introduction

The Titanic was a British-registered ship that set sail on its maiden
voyage on April 10th, 1912 with 2,240 passengers and crew on board. On
April 15th, 1912, the ship struck an iceberg, split in half, and sank to
the bottom of the ocean [@noaa2023]. In this report, we are going to
analyze the data in the Titanic.csv file and use it to determine the
best model for predicting whether someone on board would live or die. By
creating this model, we hope to understand what factors a passenger
could have taken into account in order to reduce their risk of death
during the trip. We cleaned the data and split into into a train/test
split in order to properly train our models. We created simple linear
models, multivariate linear models, logistic models (both binomial and
poisson), a regression tree, and a neural network model. The train
sample size was 916 data points (70.03%) and the test sample size was
392 data points (29.97%). We built the models after examining the data
and determining which predictor variables we thought would be most
relevant for survival rate. Once we had our variables and training data,
we created the models and examined the performance of the models on both
training and testing data to determine if they were robust. We also
examined if the model assumptions appeared to hold for each model.

::: pagebreak
:::

# Description of the data and quality

Based on the data cleaning we were able to only remove 2 rows from the
data set. We used median imputation as well as KNN for various columns.
We also dummified several categorical columns. We found that leaving
sibsp and parch as continuous as opposed to categorical increased their
contributions to the model performance (See [Appendix A](#appendix_A)).
Further, we also extracted the deck number and found that removing
deck_G from the model increased its performance.

## Loading the data

```{r}
odata <- read.csv("../data/Titanic_Survival_Data.csv")
cat("Size of entire data set:", nrow(odata), "\n")
```

## Removing un-needed columns

Name: Removing because names have no inference on surivival (inference)

ticket: Ticket No. will also likely not have an influence in survival

boat: This is highly correlated to the survival dependant variable since
people who made it on a boat likely survived

body: This is highly correlated to the survival dependant variable since
people who's body was recovered did not survive.

home.dest: The destination likely has nothing to do with the survival

```{r}
data.clean = odata[, !(names(odata) %in% c("name", "ticket", "boat","body","home.dest"))]
```

## Data Augmentation

We extracted the deck letter from the cabin since it could potentially
correlate to the survival.

```{r}
#Extract deck letter from cabin
data.clean$deck <- substr(data.clean$cabin, 1,1)
# Remove cabin col:
data.clean$cabin <- NULL
```

## Initial Check for Missing values

We see that age and deck have the most amount of missing data, therefore
we proceed to impute them.

```{r}
#| fig-cap: "Percentage of Missing Values"
print(plot_missing_barchart(data.clean))
```

## Imputing data

Below we impute Age using the median value in that column.

For deck we use KNN to impute the missing deck values.

After imputing these two columns we can see that the largest amount of
missing data is \~0.2% which is quite small and can be removed.

```{r}
# ---- Age----
#Replace NAs in age column with Median value 
set.seed (1023)
median_age <- median(data.clean$age, na.rm = TRUE)
data.clean <- data.clean %>%
  mutate(age = ifelse(is.na(age), median_age, age))

# ---- deck----
# For deck, since its a category, we decided to use KNN  to impute the column:

# Install if not already installed
# install.packages("VIM")
library(VIM)

# Replace "" with NA in the 'deck' column
data.clean$deck[data.clean$deck == ""] <- NA

# Convert 'cabin' to factor
data.clean$deck <- as.factor(data.clean$deck)

# Apply kNN imputation just to Cabin column
set.seed (1023)
data.clean <- kNN(data.clean, variable = "deck", k = 5)

# Check that NAs were imputed
# sum(is.na(data.clean$deck))        # Original
# sum(is.na(data.clean.imputed$deck)) # After

# Remove indicator col:
data.clean$deck_imp <- NULL
```

```{r}
#| fig-cap: "Percentage of Missing Values after Imputation"
################################################################################
#          Check for Missing values after Imputation                           #
################################################################################

plot_missing_barchart(data.clean)
```

## Dummifying Columns:

We dummify pclass, sex, embarked and deck. We leave sibsp and parch as
continuous variables as we observed that dummifying these columns leads
to smaller significance (See [Appendix A](#appendix_A)), whilst leaving
them as continuous maximizes their contributions to the models
explanatory power.

```{r}
# Dummifying pclass:
data.clean$pclass_1 = ifelse(data.clean$pclass == 1, 1, 0)
data.clean$pclass_2 = ifelse(data.clean$pclass == 2, 1, 0)

# Dummifying sex:
data.clean$sex_M = ifelse(data.clean$sex == 'male', 1, 0)

# Dummifying embarked:
data.clean$embarked_C = ifelse(data.clean$embarked == 'C', 1, 0)
data.clean$embarked_Q = ifelse(data.clean$embarked == 'Q', 1, 0)

# Dummifying deck:
data.clean$deck_A = ifelse(data.clean$deck == 'A', 1, 0)
data.clean$deck_B = ifelse(data.clean$deck == 'B', 1, 0)
data.clean$deck_C = ifelse(data.clean$deck == 'C', 1, 0)
data.clean$deck_D = ifelse(data.clean$deck == 'D', 1, 0)
data.clean$deck_E = ifelse(data.clean$deck == 'E', 1, 0)
data.clean$deck_F = ifelse(data.clean$deck == 'F', 1, 0)
data.clean$deck_G = ifelse(data.clean$deck == 'G', 1, 0)

# Removing Dummified cols:
data.clean = subset(data.clean, select  = -c(pclass, sex, embarked,deck))
```

## Remove NA rows and deck_G

Below we remove NA rows, which turned out to be only 2 after proper
cleaning and imputation. We also removed deck_G as we observed that it
has a large skew in the data distribution with only 13 people allocated
in this deck. It was observed that this variable lead to erroneous
predictions in the model.

```{r}
#| fig-cap: "Histogram of Deck_G"
# Plot histogram of the 'values' column
hist(data.clean$deck_G,
     main = "Histogram of Values",
     xlab = "Values",
     col = "skyblue",
     border = "white")

# Removing deck_G col:
data.clean = subset(data.clean, select  = -c(deck_G))
```

```{r}
data.clean = na.omit(data.clean)
cat(nrow(odata) - nrow(data.clean),'rows were removed from original dataset')
```

## Divide into Test / Train

Finally we divide into 70% training data and 30% test data.

```{r}
set.seed (1023)
train_indices = sample(1 : nrow(data.clean), size = 0.7005*nrow(data.clean), replace = FALSE)
train = data.clean[train_indices,]
test = data.clean[-train_indices,]
cat("We are using:", nrow(train)/nrow(data.clean) * 100, '% of the data for training')
```

## EDA

Using the training data set we use a variety of method to draw some
initial conclusions:

-   Histogram: Showing that more people in their late teens up to late
    thirties survived.
-   Bar chart showing that more people died than survived
-   Bar chart showing that a higher number of people survived when they
    had less siblings on board.
-   Correlation matrix shows that sex and Deck_F are highly negatively
    correlated to survival. There is a soft positive correlation to
    pclass_1.
-   There is a high correlation between pclass_1 and fare, this
    justifies that one of these predictors can potentially be removed.
-   The scatter plots did not give us much more information on the
    relation between the predictors and the dependent variable.

```{r}
#| fig-cap: "Histogram of survival vs age"
# Histogram showing that more people in their late teens up to late thirties survived.
ggplot(train, aes(age)) +
  geom_histogram(bins=30)
```

```{r}
#| fig-cap: "Barchart of survival"
# Bar chart showing that more people died than survived
ggplot(train, aes(survived)) +
  geom_bar()
```

```{r}
#| fig-cap: "Barchart of survival vs Num. of siblings"
# Bar chart showing that a higher number of people survived when they had less 
# siblings on board.
ggplot(train, aes(sibsp, survived)) +
  geom_bar(stat='identity')
```

```{r}
#| fig-cap: "Scatter plots of all variables in train data"
cor(train)
pairs(train[c(1:4,7,13)])
# Since this data is mainly categorical, the scatterplot and correlation matrix are not very useful.
```

[@statology2025] is used to develop the correlation values between our
categorical columns. This describes the use of pysch and rcompanion.

```{r}
#install.packages("psych")
library(psych) # [@statology2025] to understand how this works
tetrachoric(train[, c("survived", "sex_M")])
tetrachoric(train[, c("survived", "pclass_1")])
tetrachoric(train[, c("survived", "pclass_2")])

#install.packages("rcompanion")
library(rcompanion) # Reference 4 to understand how this works.
cramerV(train$survived, train$sex)
```

```{r}
#| fig-cap: "Correlation Matrix"
library(corrplot)
cor_matrix <- cor(train)#[,1]
corrplot(cor_matrix, method = "circle")
```

::: pagebreak
:::

# Model Development Process

The data was properly cleaned and divided into train/test in the prior
section.

Here we train a binary model. The Q-Q plot shows that the residuals are
indeed normally distributed so a transformation is potentially not
necessary.

The statistical comparison between test and train data shows that the
model is very stable with an accuracy of \~84% for both.

We also analyzed the VIF and we see that there is high degree of
correlation between the decks, this provides justification to remove
some of the decks as predictors.

```{r}
library(car)

# Log model on train data:
set.seed (1023)
lmod <- glm(survived ~ ., family = binomial, data = train)
# summary(lmod)
vif(lmod)

```

```{r}
#| tbl-cap: ""
y_hat_log_train <- predict(lmod, data = train, type="response")
predictions_log_train <- ifelse(y_hat_log_train > 0.5, 1, 0)

y_hat_log_test<-predict(lmod, newdata = test, type="response")
predictions_log_test <- ifelse(y_hat_log_test > 0.5, 1, 0)

confusion_matrix_log_train <- confusionMatrix(as.factor(predictions_log_train), as.factor(train$survived),mode="prec_recall", positive = "1")
base.model.accuracy = confusion_matrix_log_train$overall['Accuracy']
base.model.f1 = confusion_matrix_log_train$byClass['F1']
base.model.train.summary = data.frame(
  Accuracy = base.model.accuracy,
  F1 = base.model.f1
)
row.names(base.model.train.summary) <- 'base.model.train'  



confusion_matrix_log_test <- confusionMatrix(as.factor(predictions_log_test), as.factor(test$survived),mode="prec_recall", positive = "1")
base.model.accuracy = confusion_matrix_log_test$overall['Accuracy']
base.model.f1 = confusion_matrix_log_test$byClass['F1']
base.model.test.summary = data.frame(
  Accuracy = base.model.accuracy,
  F1 = base.model.f1
)
row.names(base.model.test.summary) <- 'base.model.test'
```

```{r}
#| tbl-cap: "Summary Stats. of base log. model"
data.frame(rbind(base.model.train.summary, base.model.test.summary))
```

```{r}
#| tbl-cap: "Summary of Base log Model using train data"
summary(lmod)
```

```{r}
#| fig-cap: "4x4 standard plots for log. model"
par(mfrow=c(2,2))
plot(lmod)
# Plots show that a linear model is not appropriate for this data.
```

::: pagebreak
:::

# Model Performance Testing

We compare four different models:

1\) Base log. model (lmod)

2\) Model with insig. pred. removed (filtered.model)

3\) Stepwise model (step.model)

4\) Model with high vif pred. removed (vif.model)

When comparing the accuracy and F1 score on all models the filtered
model was the highest performer and we decided to use that as the
champion model until now. We suspect that multicollinearity among the
decks has an impact on the model performance which is why removing these
collinear predictors and filtering for insignificant variables improved
the model slightly against the based model (+0.3%).

```{r}
################################################################################
#              Function to remove Insig. Predictors one by one                 # 
################################################################################

backward_eliminate = function(model, alpha = 0.05) {
  repeat {
    d1 = drop1(model, test = "F")
    
    # Get p-values excluding intercept row
    pvals = d1$`Pr(>F)`[-1]
    
    # Stop if all predictors are significant or only intercept left
    if( all(is.na(pvals)) || max(pvals, na.rm = TRUE) <= alpha ){
      print("all variable are signifcant")
      break
    } 
    
    # Remove the term with max p-value
    term_to_remove = rownames(d1)[-1][which.max(pvals)]
    cat("Removing:", term_to_remove, "with p-value", max(pvals, na.rm = TRUE), "\n")
    model = update(model, paste(". ~ . -", term_to_remove))
  }
  return(model)
}
```

```{r}
################################################################################
#                     Function to remove Cooks Outliers                        # 
################################################################################                        
# model_formula: A formula object, e.g. PSA.level ~ .
# data: A data frame containing the variables in the model.
# threshold: A numeric value indicating the Cook's D threshold (default 0.5).
# print: If TRUE (default) will print the rows beign removed.
# returns: A list with the final model and the filtered dataset.
# 
# Example usage:
# result = remove_cooks_outliers(PSA.level ~ ., mydata)
# summary(result$model)
# str(result$filtered_data)
remove_cooks_outliers = function(model_formula, data, threshold = 0.5, 
                                 print = TRUE) 
  {
  
  all_high_cd_rows = data.frame()  # to store all removed rows

  repeat {
    model = glm(model_formula, family = binomial, data = data) 
    cooksD = cooks.distance(model)
    high_cd_indices = which(cooksD > threshold)
    
    if (length(high_cd_indices) == 0) { # If there are no more outliers
      break
    }
    
    if (print == TRUE){
      cat("Removing rows with Cook's D >", threshold, ":\n", high_cd_indices, "\n")
    }
    # Save these outliers before removing them
    high_cd_rows = data[high_cd_indices, ]
    all_high_cd_rows = rbind(all_high_cd_rows, high_cd_rows)
    
    # Update data by removing high CD rows for next iteration.
    data = data[-high_cd_indices, ]
  }
  
  final_model = glm(model_formula, family = binomial, data = data)
  return(list(model = final_model, filtered_data = data, high_cd_data = all_high_cd_rows))
}
```

```{r}
################################################################################
#                    Model with insig. variables removed                      # 
################################################################################
binary.model.filtered = backward_eliminate(lmod)

################################################################################
#                step wise model with insig. variables removed                 # 
################################################################################
library(olsrr)

ols_step_both_p(lmod,p_enter=0.1,p_remove=0.05,details=FALSE)

# Fit model with stepwise parms only:
binary.model.stepwise = glm(survived ~ sex_M + deck_F  + age + deck_E  + 
                              pclass_2 + pclass_1 + sibsp + fare, 
                            family = binomial, data = train)
summary(binary.model.stepwise)

# Removing fare from stepwise model since its insig.:
binary.model.stepwise = update(binary.model.stepwise, paste(". ~ . -", 'fare'))
summary(binary.model.stepwise)

################################################################################
#                Model with influential points removed                         # 
################################################################################
# Applying function to remove influential points via Cooks Distance:
filtering.result = remove_cooks_outliers(survived ~ ., data = train)
cat(nrow(filtering.result$high_cd_data),'rows were identified as outliers')

# plot(logmod)

# Since there were now rows identified as outliers, then this model will be the 
# same as the initial binary model and need no be considered in the final 
# model compare.

################################################################################
#                Model with high VIF preds removed                             # 
################################################################################
library(car)
vif(lmod)

vif.model = glm(survived ~ . -deck_F, family = binomial, data = train)
vif(vif.model)

# Removing deck_F from the model eliminates the multicollinearity completely. 
```

```{r}
################################################################################
#                         Function to calc. cutoff                             # 
################################################################################
cutoff.prg<-function(pred,act){
# pred<-predicted_probabilities
# act<-true_labels
p<-seq(0,1,0.01)
n<-length(p)
out<-matrix(0,nrow=n,ncol=12)
for(i in 1:n){
predictions <- ifelse(pred >p[i], 1, 0)
confusion_matrix <- confusionMatrix(as.factor(predictions),as.factor(act),mode="prec_recall", positive = "1")
out[i,]<-cbind(p=p[i],t(confusion_matrix[[4]]))
}
dimnames(out)[[2]]<-c("p","Sensitivity","Specificity","Pos Pred Value","Neg Pred Value","Precision","Recall", "F1","Prevalence","Detection Rate","Detection Prevalence", "Balanced Accuracy")
out
}
```

```{r}
# Finding the optimal cutoff
observations = train$survived
prob <- predict(lmod, train, type="response")

test_cutoff<-cutoff.prg(prob,observations)
plot(test_cutoff[,1],test_cutoff[,8],xlab="cut off",ylab="F1")
optimal.cutoff = test_cutoff[which.max(test_cutoff[,8]),]
optimal.cutoff[1]
```

```{r}
observations.train = train$survived
observations.test = test$survived

# Confusion matrix on base log model on train data
y_hat_prob = predict(lmod, train, type="response")
predictions.binary.model.train <- ifelse(y_hat_prob > optimal.cutoff[1], 1, 0)
confusion.matrix.binary.model.train <- confusionMatrix(as.factor(predictions.binary.model.train),as.factor(observations.train),mode="prec_recall", positive = "1")
base.model.accuracy = confusion.matrix.binary.model.train$overall['Accuracy']
base.model.f1 = confusion.matrix.binary.model.train$byClass['F1']
base.model.train.summary = data.frame(
  Accuracy = base.model.accuracy,
  F1 = base.model.f1
)
row.names(base.model.train.summary) <- 'base.model.train'

# Confusion matrix on base log model on test data
y_hat_prob = predict(lmod, test, type="response")

predictions.binary.model.test <- ifelse(y_hat_prob > optimal.cutoff[1], 1, 0)

confusion.matrix.binary.model.test <- confusionMatrix(as.factor(predictions.binary.model.test),as.factor(observations.test), positive = "1")

base.model.accuracy.test = confusion.matrix.binary.model.test$overall['Accuracy']
base.model.f1.test = confusion.matrix.binary.model.test$byClass['F1']

base.model.test.summary = data.frame(
  Accuracy = base.model.accuracy.test,
  F1 = base.model.f1.test
)
row.names(base.model.test.summary) <- 'base.model.test'

# Confusion matrix on stepwise log model on train data
y_hat_prob = predict(binary.model.stepwise, train, type="response")
predictions.binary.model.step.train <- ifelse(y_hat_prob > optimal.cutoff[1], 1, 0)
confusion.matrix.binary.model.step.train <- confusionMatrix(as.factor(predictions.binary.model.step.train),as.factor(observations.train),mode="prec_recall", positive = "1")
stepwise.model.accuracy = confusion.matrix.binary.model.step.train$overall['Accuracy']
stepwise.model.f1 = confusion.matrix.binary.model.step.train$byClass['F1']
stepwise.model.train.summary = data.frame(
  Accuracy = stepwise.model.accuracy,
  F1 = stepwise.model.f1
)
row.names(stepwise.model.train.summary) <- 'stepwise.model.train'

# Confusion matrix on stepwise log model on test data
y_hat_prob = predict(binary.model.stepwise, test, type="response")
predictions.binary.model.step.test <- ifelse(y_hat_prob > optimal.cutoff[1], 1, 0)
confusion.matrix.binary.model.step.test <- confusionMatrix(as.factor(predictions.binary.model.step.test),as.factor(observations.test),mode="prec_recall", positive = "1")
stepwise.model.accuracy = confusion.matrix.binary.model.step.test$overall['Accuracy']
stepwise.model.f1 = confusion.matrix.binary.model.step.test$byClass['F1']
stepwise.model.test.summary = data.frame(
  Accuracy = stepwise.model.accuracy,
  F1 = stepwise.model.f1
)
row.names(stepwise.model.test.summary) <- 'stepwise.model.test'

# Confusion matrix on log model w/ insig. pred. removed
y_hat_prob = predict(binary.model.filtered, train, type="response")
predictions.binary.model.filtered.train <- ifelse(y_hat_prob > optimal.cutoff[1], 1, 0)
confusion.matrix.binary.model.filtered.train <- confusionMatrix(as.factor(predictions.binary.model.filtered.train),as.factor(observations.train),mode="prec_recall", positive = "1")
filtered.model.accuracy = confusion.matrix.binary.model.filtered.train$overall['Accuracy']
filtered.model.f1 = confusion.matrix.binary.model.filtered.train$byClass['F1']
filtered.model.train.summary = data.frame(
  Accuracy = filtered.model.accuracy,
  F1 = filtered.model.f1
)
row.names(filtered.model.train.summary) <- 'filtered.model.train'

# Confusion matrix on log model w/ insig. pred. removed
y_hat_prob = predict(binary.model.filtered, test, type="response")

predictions.binary.model.filtered.test <- ifelse(y_hat_prob > optimal.cutoff[1], 1, 0)

confusion.matrix.binary.model.filtered.test <- confusionMatrix(as.factor(predictions.binary.model.filtered.test),as.factor(observations.test),mode="prec_recall", positive = "1")

filtered.model.accuracy = confusion.matrix.binary.model.filtered.test$overall['Accuracy']
filtered.model.f1 = confusion.matrix.binary.model.filtered.test$byClass['F1']

filtered.model.test.summary = data.frame(
  Accuracy = filtered.model.accuracy,
  F1 = filtered.model.f1
)
row.names(filtered.model.test.summary) <- 'filtered.model.test'

# Confusion matrix on log model w/ high vif pred. removed
y_hat_prob = predict(vif.model, train, type="response")
predictions.binary.model.vif.train <- ifelse(y_hat_prob > optimal.cutoff[1], 1, 0)
confusion.matrix.binary.model.vif.train <- confusionMatrix(as.factor(predictions.binary.model.vif.train),as.factor(observations.train),mode="prec_recall", positive = "1")
vif.model.accuracy = confusion.matrix.binary.model.vif.train$overall['Accuracy']
vif.model.f1 = confusion.matrix.binary.model.vif.train$byClass['F1']
vif.model.train.summary = data.frame(
  Accuracy = vif.model.accuracy,
  F1 = vif.model.f1
)
row.names(vif.model.train.summary) <- 'vif.model.train'

# Confusion matrix on log model w/ high vif pred. removed
y_hat_prob = predict(vif.model, test, type="response")
predictions.binary.model.vif.test <- ifelse(y_hat_prob > optimal.cutoff[1], 1, 0)
confusion.matrix.binary.model.vif.test <- confusionMatrix(as.factor(predictions.binary.model.vif.test),as.factor(observations.test),mode="prec_recall", positive = "1")
vif.model.accuracy = confusion.matrix.binary.model.vif.test$overall['Accuracy']
vif.model.f1 = confusion.matrix.binary.model.vif.test$byClass['F1']
vif.model.test.summary = data.frame(
  Accuracy = vif.model.accuracy,
  F1 = vif.model.f1
)
row.names(vif.model.test.summary) <- 'vif.model.test'
```

```{r}
#| tbl-cap: "Comparison of all log models performance"
data.frame(rbind(base.model.train.summary, base.model.test.summary, 
                 stepwise.model.train.summary, stepwise.model.test.summary,
                 filtered.model.train.summary, filtered.model.test.summary,
                 vif.model.train.summary,vif.model.test.summary))
```

::: pagebreak
:::

# Challenger Models

We decided to build a Poisson model for our challenger model. After
building our original poisson model, we first check to see if the
variables are important.

Ho: Variables are not important

Ha: Variable are important

Since we have a p-value of 0, we reject the null hypothesis. Variables
are important.

Our poisson model has three significant variables, pclass_2, sex_M, and
deck_F with an alpha of 0.05.

According to the poisson regression:

The odds of survival increases by 37.6% when the passenger is second
class. ((exp(0.31918) - 1) \* 100)

The odds of survival decreases by 68.14% when the passenger is male.
((exp(-1.144) - 1) \* 100)

The odds of survival decreases by 64.76% when the passenger is from deck
G. ((exp(-1.043) - 1) \* 100)

Our Poisson Regression has an accuracy of 74.74% with optimal cutoﬀ(.17)
based on max F1 score (0.71954).

```{r poisson}
library(MASS)

# Train base Poisson model:
poissonReg_full <- glm(survived ~ .,family=poisson, train)

# compare the fitted model to the null model and calculate if the variables are important
summary(poissonReg_full)
1-pchisq(672.00-409.95, length(coef(poissonReg_full)) - 1)

# We then reduce the model and see if the removed variables are significant
poissonReg_reduced <- glm(survived ~ pclass_2+sex_M+deck_F,family=poisson, train)

# Verify that all variables are significant and no more can be dropped
anova(poissonReg_reduced, poissonReg_full, test="Chi")
drop1(poissonReg_reduced,test="Chi")

### Train Data

# Testing against train data
predicted_probs <- predict(poissonReg_reduced, newdata = train, type = "response")

# Get the results of different cutoff values
trainResult<-cutoff.prg(predicted_probs,train$survived)

# We get get the optimal p cut off value based on the maximum F1 score
plot(trainResult[,1],trainResult[,8],xlab="cut off",ylab="F1")
poisson.model.cutoff = trainResult[which.max(trainResult[,8]),]

# Get the F1 and Accuracy for train data using the optimal p value
observations.train <- train$survived
predictions.poisson.model.train <- ifelse(predicted_probs > poisson.model.cutoff[1], 1, 0)

confusion.matrix.binary.model.poisson.train <- confusionMatrix(as.factor(predictions.poisson.model.train),as.factor(observations.train), positive = "1")

poisson.model.accuracy.train = confusion.matrix.binary.model.poisson.train$overall['Accuracy']
poisson.model.f1.train = confusion.matrix.binary.model.poisson.train$byClass['F1']

poisson.model.train.summary = data.frame(
  Accuracy = poisson.model.accuracy.train,
  F1 = poisson.model.f1.train
)

row.names(poisson.model.train.summary) <- 'poisson.model.train'

### Test Data

# Testing against test data
predicted_probs <- predict(poissonReg_reduced, newdata = test, type = "response")

# Get the F1 and Accuracy for test data using the optimal p value
observations.test <- test$survived
predictions.poisson.model.test <- ifelse(predicted_probs > poisson.model.cutoff[1], 1, 0)

confusion.matrix.binary.model.poisson.test <- confusionMatrix(as.factor(predictions.poisson.model.test),as.factor(observations.test), positive = "1")

poisson.model.accuracy.test = confusion.matrix.binary.model.poisson.test$overall['Accuracy']
poisson.model.f1.test = confusion.matrix.binary.model.poisson.test$byClass['F1']

poisson.model.test.summary = data.frame(
  Accuracy = poisson.model.accuracy.test,
  F1 = poisson.model.f1.test
)

row.names(poisson.model.test.summary) <- 'poisson.model.test'

data.frame(rbind(poisson.model.train.summary,poisson.model.test.summary ))
```

::: pagebreak
:::

# Model Limitation and Assumptions

The champion model selected is the filtered logistic regression model
due to its superior test accuracy (84.1%) and F1 score (0.78014). The
filtered logistic regression model was developed through backward
elimination procedure, applied to the full base model. This process
iteratively removed predictors with high p-values until only
statistically significant predictors remained. The base model is used as
the benchmark due to its similar performance and broader feature set.

```{r}
summary(binary.model.filtered)
```

To further evaluate and benchmark the logistic regression models, we
computed RMSE, R², and MAE using the predicted probabilities against the
actual binary outcomes. While traditional R² is not appropriate for
logistic models, these metrics provide insight into how well the
predicted probabilities align with observed outcomes. The Filtered model
appears to have higher R² (0.4425\>0.4342), lower RMSE(0.3945 \< 0.3977)
and MAE (0.1556 \<0.1582). Additionally, model validation was done
through train/test split with consistent performance across sets. The
performance on test data and train data show minimal difference,
indicating the model is robust, no over-itting problem.

To evaluate the stability and fit of the base and filtered logistic
regression models, we conducted residual-based tests on their linear
analogs. While logistic regression does not formally require normally
distributed or homoscedastic residuals, these tests offer insights into
model specification quality and residual behavior.

Breusch-Pagan Test: Both models exhibit statistically significant
heteroskedasticity (p \< 0.01)

Shapiro-Wilk Test: Both models show highly significant deviations from
normality (p \< 0.01)

Both models show similar residual behavior, with slight
heteroskedasticity and non-normality. These findings do not violate
logistic regression assumptions but suggest the model could be further
improved through additional transformations or interaction terms.

Multicollinearity was significantly lower in the filtered model (max
VIF: 3.69 vs. 16.63), supporting better generalization and
interpretability.

```{r Hao Code Model Review}
library(car)

# Normality Test
# Breusch Pagan Test on Logistic Model
BPtest_basemodel <- ols_test_breusch_pagan(lmod)
BPtest_filtered.model <- ols_test_breusch_pagan(binary.model.filtered)

# Shapiro-Wilk test
SWtest_basemodel <- shapiro.test(residuals(lmod))
SWtest_filteredmodel <- shapiro.test(residuals(binary.model.filtered))


# Combine into unified rows
diagnostics_summary <- data.frame(
  Model = c("Base.model", "Filtered.model"),
  BP_Statistic = c(BPtest_basemodel$bp, BPtest_filtered.model$bp),
  BP_pvalue = c(BPtest_basemodel$p, BPtest_filtered.model$p),
  Shapiro_W = c(SWtest_basemodel$statistic, SWtest_filteredmodel$statistic),
  Shapiro_pvalue = c(SWtest_basemodel$p.value, SWtest_filteredmodel$p.value)
)
```

```{r}
#| tbl-cap: "Summary of Diagnostic tests on Filtered vs Base models"
diagnostics_summary
```

```{r}
#| tbl-cap: "VIF Results for Base Logistic Regression Model"
# Run VIF on the logistic model
ols_vif_tol(lmod)
```

```{r}
#| tbl-cap: "VIF Results for Filtered Logistic Regression Model"
ols_vif_tol(binary.model.filtered)
```

```{r}
# Base Model - Train
observations.train <- train$survived
y_hat_base_train <- predict(lmod, train, type = "response")
pred_base_train <- ifelse(y_hat_base_train > optimal.cutoff[1], 1, 0)
ModelTrain_base <- data.frame(obs = observations.train, pred = pred_base_train)
log.train.base <- defaultSummary(ModelTrain_base)

# Base Model - Test
observations.test <- test$survived
y_hat_base_test <- predict(lmod, test, type = "response")
pred_base_test <- ifelse(y_hat_base_test > optimal.cutoff[1], 1, 0)
ModelTest_base <- data.frame(obs = observations.test, pred = pred_base_test)
log.test.base <- defaultSummary(ModelTest_base)

# Base Model - Train
observations.train <- train$survived
y_hat_base_train <- predict(binary.model.filtered, train, type = "response")
pred_base_train <- ifelse(y_hat_base_train > optimal.cutoff[1], 1, 0)
ModelTrain_base <- data.frame(obs = observations.train, pred = pred_base_train)
filtered.log.train.base <- defaultSummary(ModelTrain_base)

# Base Model - Test
observations.test <- test$survived
y_hat_base_test <- predict(binary.model.filtered, test, type = "response")
pred_base_test <- ifelse(y_hat_base_test > optimal.cutoff[1], 1, 0)
ModelTest_base <- data.frame(obs = observations.test, pred = pred_base_test)
filtered.test.base <- defaultSummary(ModelTest_base)
```

```{r}
#| tbl-cap: "Comparison of stats between base and filtered log models:"
data.frame(rbind(log.train.base,log.test.base,filtered.log.train.base,filtered.test.base))
  
```

::: pagebreak
:::

# Ongoing Model Monitoring Plan

In order to maintain the effectiveness of the model, we would need to
continue to test it on new data. Since the Titanic was a rare event, we
do not have a lot of new data to test on the model, but we can still be
prepared in case new data were to become available. The first step in
monitoring the model is to determine specific thresholds that we expect
the model to stay above. We would want the model to maintain certain
Accuracy and F1 values in order to determine that the model is working
correctly. The first level we are monitoring is Accuracy or F1 values
falling below 70%. Once one of those values falls below 70% we would
examine the test data to make sure the data is valid. Assuming the data
is valid, we will keep a closer eye on the model performance moving
forward. Once both Accuracy and F1 values fall below 70%, or one of the
values falls below 60%, we will retrain the model on more recent data.

One of the biggest concerns with our model is data drift. Since the
Titanic sank over 100 years ago, the data that we are using from the
model may not align with data relevant to ship travel today. Our
expectation is that age is one data point that will look drastically
different for ship travel today than it did for the Titanic simply due
to the massive increase in life expectancy [@verywellhealth2024]. Due to
this change, we will make sure to closely monitor age in new data and
will adjust the model if we see the median age change by 20%.

Another concern of ours is that there will be concept drift. It is
possible that over time, the relationships between variables in the
model will change, resulting in the model losing effectiveness. In order
to account for this, we will continue to monitor correlations between
the model variables and will adjust the model if we notice any large
(greater than 20%) changes in the correlations.

Overall, we will monitor our model to make sure it remains robust. Our
assumptions are that a robust model will maintain both Accuracy and F1
values of over 70%, while the correlations in the model remain fairly
constant and new data doesn't appear to vary drastically from our
training data. We will look for the model to keep Accuracy and F1 values
above 70%, while also making sure that median age of new data doesn't
change by more than 20% and that correlations between variables in the
model don't change by more than 20%. If any of these events were to
happen, we would look to retrain our model on more recent data in order
to maintain model robustness.

::: pagebreak
:::

# Conclusion

As we can see from the comparing all three models. The "filtered.model"
performed the best against the test data set. This is due to the removal
insignificant variables until our model remained with only sigificant
predictors. Multicollinarity seemed to be a big factor that impacted our
models accuracy and F1. Once we removed indications multicollinarity, we
could see that our models performance increased over the base model.

However, with the "poisson.model", we see a decrease in accuracy
compared the base model. This decrease is expected due to the nature of
the poisson regression. The regression is most useful when the response
variable is a count variable rather than a binary response. Thus a
decrease in accuracy and F1 is expected against the base model.

As a conclusion, the model reduced via p-values filtering is the best
model to predict if a passenger were to survive, with an accuracy of
84.28% and improved over the base model by 0.25%.

```{r conclusion}
#| tbl-cap: "Comparison of our base model, best performing developed model, and challenger model"
data.frame(rbind(base.model.test.summary, 
                 filtered.model.test.summary,
                 poisson.model.test.summary))
  
```

::: pagebreak
:::

# Appendix A: Check if 'sibsp' and 'parch' should be continuous or categorical {#appendix_A .unnumbered .toc-entry}

We don't see significant improvement between modeling these predictors
as continuous or categorical, therefore we decided to leave them as
continuous.

```{r Group model creation code - sibsp and parch as individual categories}
library(car)
data.clean.ap1 = odata[, !(names(odata) %in% c("name", "ticket", "boat","body","home.dest"))]

################################################################################
#                           Data Augmentation                                  #   
################################################################################
#Extract deck letter from cabin
data.clean.ap1$deck <- substr(data.clean.ap1$cabin, 1,1)

# Remove cabin col:
data.clean.ap1$cabin <- NULL

################################################################################
#                           Imputing data                                      #   
################################################################################

# ---- Age----
#Replace NAs in age column with Median value 
median_age <- median(data.clean.ap1$age, na.rm = TRUE)
data.clean.ap1 <- data.clean.ap1 %>%
  mutate(age = ifelse(is.na(age), median_age, age))

# ---- deck----
# For deck, since its a category, we decided to use KNN  to impute the column:

# Install if not already installed
# install.packages("VIM")
library(VIM)

# Replace "" with NA in the 'deck' column
data.clean.ap1$deck[data.clean.ap1$deck == ""] <- NA

# Convert 'cabin' to factor
data.clean.ap1$deck <- as.factor(data.clean.ap1$deck)

# Apply kNN imputation just to Cabin column
data.clean.ap1 <- kNN(data.clean.ap1, variable = "deck", k = 5)

# Check that NAs were imputed
# sum(is.na(data.clean$deck))        # Original
# sum(is.na(data.clean.imputed$deck)) # After

# Remove indicator col:
data.clean.ap1$deck_imp <- NULL

################################################################################
#                           Dummify Cat. cols                                  # 
################################################################################

# Dummifying pclass:
data.clean.ap1$pclass_1 = ifelse(data.clean.ap1$pclass == 1, 1, 0)
data.clean.ap1$pclass_2 = ifelse(data.clean.ap1$pclass == 2, 1, 0)

# Dummifying sex:
data.clean.ap1$sex_M = ifelse(data.clean.ap1$sex == 'male', 1, 0)

# Dummifying embarked:
data.clean.ap1$embarked_C = ifelse(data.clean.ap1$embarked == 'C', 1, 0)
data.clean.ap1$embarked_Q = ifelse(data.clean.ap1$embarked == 'Q', 1, 0)

# Dummifying deck:
data.clean.ap1$deck_A = ifelse(data.clean.ap1$deck == 'A', 1, 0)
data.clean.ap1$deck_B = ifelse(data.clean.ap1$deck == 'B', 1, 0)
data.clean.ap1$deck_C = ifelse(data.clean.ap1$deck == 'C', 1, 0)
data.clean.ap1$deck_D = ifelse(data.clean.ap1$deck == 'D', 1, 0)
data.clean.ap1$deck_E = ifelse(data.clean.ap1$deck == 'E', 1, 0)
data.clean.ap1$deck_F = ifelse(data.clean.ap1$deck == 'F', 1, 0)
#data.clean.ap1$deck_G = ifelse(data.clean.ap1$deck == 'G', 1, 0) # removed due to causing issues

# Dummifying sibsp:
data.clean.ap1$sibsp_1 = ifelse(data.clean.ap1$sibsp == 1, 1, 0)
data.clean.ap1$sibsp_2 = ifelse(data.clean.ap1$sibsp == 2, 1, 0)
data.clean.ap1$sibsp_3 = ifelse(data.clean.ap1$sibsp == 3, 1, 0)
data.clean.ap1$sibsp_4 = ifelse(data.clean.ap1$sibsp == 4, 1, 0)
data.clean.ap1$sibsp_5 = ifelse(data.clean.ap1$sibsp == 5, 1, 0)
#data.clean.ap1$sibsp_8 = ifelse(data.clean.ap1$sibsp == 8, 1, 0) # removed due to causing issues

# Dummifying parch:
data.clean.ap1$parch_1 = ifelse(data.clean.ap1$parch == 1, 1, 0)
data.clean.ap1$parch_2 = ifelse(data.clean.ap1$parch == 2, 1, 0)
data.clean.ap1$parch_3 = ifelse(data.clean.ap1$parch == 3, 1, 0)
data.clean.ap1$parch_4 = ifelse(data.clean.ap1$parch == 4, 1, 0)
data.clean.ap1$parch_5 = ifelse(data.clean.ap1$parch == 5, 1, 0)
data.clean.ap1$parch_6 = ifelse(data.clean.ap1$parch == 6, 1, 0)
#data.clean.ap1$parch_9 = ifelse(data.clean.ap1$parch == 9, 1, 0) # removed due to causing issues

# Removing Dummified cols:
data.clean.ap1 = subset(data.clean.ap1, select  = -c(pclass, sex, embarked, deck))#, sibsp, parch))

data.clean.ap1 = na.omit(data.clean.ap1)

cat(nrow(odata) - nrow(data.clean.ap1),'rows were removed from original dataset')

set.seed(567)
train_indices_ap1 = sample(1 : nrow(data.clean.ap1), size = 0.7005*nrow(data.clean.ap1), replace = FALSE)
train.ap1 = data.clean.ap1[train_indices_ap1,]
test.ap1 = data.clean.ap1[-train_indices_ap1,]
cat("We are using:", nrow(train.ap1)/nrow(data.clean.ap1) * 100, '% of the data for training')

mulvar_model.ap1 <- lm(survived ~ ., data = train.ap1)
summary(mulvar_model.ap1)
vif(mulvar_model.ap1)

lmod.ap1 <- glm(as.factor(survived) ~ ., family = binomial, data = train.ap1)
summary(lmod.ap1)
vif(lmod.ap1)

y_hat_mulvar_train.ap1<-predict(mulvar_model.ap1, data = train.ap1)
predictions_train.ap1 <- ifelse(y_hat_mulvar_train.ap1 > 0.5, 1, 0)
ModelTrain_mulvar.ap1<-data.frame(obs = train.ap1$survived, pred=predictions_train.ap1)
linear.train.ap1 <- defaultSummary(ModelTrain_mulvar.ap1)

y_hat_mulvar_test.ap1<-predict(mulvar_model.ap1, newdata = test.ap1)
predictions_test.ap1 <- ifelse(y_hat_mulvar_test.ap1 > 0.5, 1, 0)
ModelTest_mulvar.ap1<-data.frame(obs = test.ap1$survived, pred=predictions_test.ap1)
linear.test.ap1 <- defaultSummary(ModelTest_mulvar.ap1)


y_hat_log_train.ap1<-predict(lmod.ap1, data = train.ap1)
predictions_log_train.ap1 <- ifelse(y_hat_log_train.ap1 > 0.5, 1, 0)
ModelTrain_lmod.ap1<-data.frame(obs = train.ap1$survived, pred=predictions_log_train.ap1)
log.train.ap1 <- defaultSummary(ModelTrain_lmod.ap1)

y_hat_log_test.ap1<-predict(lmod.ap1, newdata = test.ap1)
predictions_log_test.ap1 <- ifelse(y_hat_log_test.ap1 > 0.5, 1, 0)
ModelTest_lmod.ap1<-data.frame(obs = test.ap1$survived, pred=predictions_log_test.ap1)
log.test.ap1 <- defaultSummary(ModelTest_lmod.ap1)

data.frame(rbind(linear.train.ap1,linear.test.ap1,log.train.ap1,log.test.ap1))


confusion_matrix_mulvar_train.ap1 <- confusionMatrix(as.factor(predictions_train.ap1), as.factor(train.ap1$survived),mode="prec_recall", positive = "1")
confusion_matrix_mulvar_train.ap1

confusion_matrix_mulvar_test.ap1 <- confusionMatrix(as.factor(predictions_test.ap1), as.factor(test.ap1$survived),mode="prec_recall", positive = "1")
confusion_matrix_mulvar_test.ap1  

confusion_matrix_log_train.ap1 <- confusionMatrix(as.factor(predictions_log_train.ap1), as.factor(train.ap1$survived),mode="prec_recall", positive = "1")
confusion_matrix_log_train.ap1

confusion_matrix_log_test.ap1 <- confusionMatrix(as.factor(predictions_log_test.ap1), as.factor(test.ap1$survived),mode="prec_recall", positive = "1")
confusion_matrix_log_test.ap1
```

```{r Group model creation code - sibsp and parch as 2 categories}
library(car)
data.clean.ap2 = odata[, !(names(odata) %in% c("name", "ticket", "boat","body","home.dest"))]

################################################################################
#                           Data Augmentation                                  #   
################################################################################
#Extract deck letter from cabin
data.clean.ap2$deck <- substr(data.clean.ap2$cabin, 1,1)

# Remove cabin col:
data.clean.ap2$cabin <- NULL

################################################################################
#                           Imputing data                                      #   
################################################################################

# ---- Age----
#Replace NAs in age column with Median value 
median_age <- median(data.clean.ap2$age, na.rm = TRUE)
data.clean.ap2 <- data.clean.ap2 %>%
  mutate(age = ifelse(is.na(age), median_age, age))

# ---- deck----
# For deck, since its a category, we decided to use KNN  to impute the column:

# Install if not already installed
# install.packages("VIM")
library(VIM)

# Replace "" with NA in the 'deck' column
data.clean.ap2$deck[data.clean.ap2$deck == ""] <- NA

# Convert 'cabin' to factor
data.clean.ap2$deck <- as.factor(data.clean.ap2$deck)

# Apply kNN imputation just to Cabin column
data.clean.ap2 <- kNN(data.clean.ap2, variable = "deck", k = 5)

# Check that NAs were imputed
# sum(is.na(data.clean$deck))        # Original
# sum(is.na(data.clean.imputed$deck)) # After

# Remove indicator col:
data.clean.ap2$deck_imp <- NULL


################################################################################
#                           Dummify Cat. cols                                  # 
################################################################################

# Dummifying pclass:
data.clean.ap2$pclass_1 = ifelse(data.clean.ap2$pclass == 1, 1, 0)
data.clean.ap2$pclass_2 = ifelse(data.clean.ap2$pclass == 2, 1, 0)

# Dummifying sex:
data.clean.ap2$sex_M = ifelse(data.clean.ap2$sex == 'male', 1, 0)

# Dummifying embarked:
data.clean.ap2$embarked_C = ifelse(data.clean.ap2$embarked == 'C', 1, 0)
data.clean.ap2$embarked_Q = ifelse(data.clean.ap2$embarked == 'Q', 1, 0)

# Dummifying deck:
data.clean.ap2$deck_A = ifelse(data.clean.ap2$deck == 'A', 1, 0)
data.clean.ap2$deck_B = ifelse(data.clean.ap2$deck == 'B', 1, 0)
data.clean.ap2$deck_C = ifelse(data.clean.ap2$deck == 'C', 1, 0)
data.clean.ap2$deck_D = ifelse(data.clean.ap2$deck == 'D', 1, 0)
data.clean.ap2$deck_E = ifelse(data.clean.ap2$deck == 'E', 1, 0)
data.clean.ap2$deck_F = ifelse(data.clean.ap2$deck == 'F', 1, 0)
#data.clean.ap2$deck_G = ifelse(data.clean.ap2$deck == 'G', 1, 0) # removed due to causing issues

# Dummifying sibsp to 2 categories:
data.clean.ap2$sibsp_y = ifelse(data.clean.ap2$sibsp > 0, 1, 0)

# Dummifying parch to 2 categories:
data.clean.ap2$parch_y = ifelse(data.clean.ap2$parch > 0, 1, 0)

# Removing Dummified cols:
data.clean.ap2 = subset(data.clean.ap2, select  = -c(pclass, sex, embarked, deck))#, sibsp, parch))

data.clean.ap2 = na.omit(data.clean.ap2)

cat(nrow(odata) - nrow(data.clean.ap2),'rows were removed from original dataset')

set.seed(567)
train_indices_ap2 = sample(1 : nrow(data.clean.ap2), size = 0.7005*nrow(data.clean.ap2), replace = FALSE)
train.ap2 = data.clean.ap2[train_indices_ap2,]
test.ap2 = data.clean.ap2[-train_indices_ap2,]
cat("We are using:", nrow(train.ap2)/nrow(data.clean.ap2) * 100, '% of the data for training')

mulvar_model.ap2 <- lm(survived ~ ., data = train.ap2)
summary(mulvar_model.ap2)
vif(mulvar_model.ap2)

lmod.ap2 <- glm(as.factor(survived) ~ ., family = binomial, data = train.ap2)
summary(lmod.ap2)
vif(lmod.ap2)

y_hat_mulvar_train.ap2<-predict(mulvar_model.ap2, data = train.ap2)
predictions_train.ap2 <- ifelse(y_hat_mulvar_train.ap2 > 0.5, 1, 0)
ModelTrain_mulvar.ap2<-data.frame(obs = train.ap2$survived, pred=predictions_train.ap2)
linear.train.ap2 <- defaultSummary(ModelTrain_mulvar.ap2)

y_hat_mulvar_test.ap2<-predict(mulvar_model.ap2, newdata = test.ap2)
predictions_test.ap2 <- ifelse(y_hat_mulvar_test.ap2 > 0.5, 1, 0)
ModelTest_mulvar.ap2<-data.frame(obs = test.ap2$survived, pred=predictions_test.ap2)
linear.test.ap2 <- defaultSummary(ModelTest_mulvar.ap2)


y_hat_log_train.ap2<-predict(lmod.ap2, data = train.ap2)
predictions_log_train.ap2 <- ifelse(y_hat_log_train.ap2 > 0.5, 1, 0)
ModelTrain_lmod.ap2<-data.frame(obs = train.ap2$survived, pred=y_hat_log_train.ap2)
log.train.ap2 <- defaultSummary(ModelTrain_mulvar.ap2)

y_hat_log_test.ap2<-predict(lmod.ap2, newdata = test.ap2)
predictions_log_test.ap2 <- ifelse(y_hat_log_test.ap2 > 0.5, 1, 0)
ModelTest_lmod.ap2<-data.frame(obs = test.ap2$survived, pred=predictions_log_test.ap2)
log.test.ap2 <- defaultSummary(ModelTest_lmod.ap2)


data.frame(rbind(linear.train.ap2,linear.test.ap2,log.train.ap2,log.test.ap2))


confusion_matrix_mulvar_train.ap2 <- confusionMatrix(as.factor(predictions_train.ap2), as.factor(train.ap2$survived),mode="prec_recall", positive = "1")
confusion_matrix_mulvar_train.ap2

confusion_matrix_mulvar_test.ap2 <- confusionMatrix(as.factor(predictions_test.ap2), as.factor(test.ap2$survived),mode="prec_recall", positive = "1")
confusion_matrix_mulvar_test.ap2

confusion_matrix_log_train.ap2 <- confusionMatrix(as.factor(predictions_log_train.ap2), as.factor(train.ap2$survived),mode="prec_recall", positive = "1")
confusion_matrix_log_train.ap2

confusion_matrix_log_test.ap2 <- confusionMatrix(as.factor(predictions_log_test.ap2), as.factor(test.ap2$survived),mode="prec_recall", positive = "1")
confusion_matrix_log_test.ap2
```

# References
