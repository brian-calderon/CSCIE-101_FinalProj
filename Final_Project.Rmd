---
title: "Group Project"
output: pdf_document
date: "2025-04-25"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages("fastDummies")
library(ISLR2)
library(tidyverse)
library(caret)
library(MASS)
library(fastDummies)
library(stringr)
<<<<<<< HEAD
=======
odata<- read.csv("./template/Titanic_Survival_Data.csv")
cat("Size of entire data set:", nrow(odata), "\n")

#Remove Unneeded Columns'
data <- odata[, !(names(odata) %in% c("name", "ticket", "boat","body","home.dest"))]

#Replace NAs in age column with Median value 
median_age <- median(data$age, na.rm = TRUE)
data <- data %>%
  mutate(age = ifelse(is.na(age), median_age, age))

#Extract deck letter from cabin
data$deck <- substr(data$cabin, 1,1)
data$cabin <- NULL

# Convert into factors
data$deck <- factor(data$deck, levels = c("A", "B", "C", "D", "E", "F", "G", "T"))
data$sex <- factor(data$sex, levels = c("female", "male"))
data$embarked <- factor(data$embarked, levels = c("C", "Q", "S"))

# Remove rows with NA in any of the categorical columns in order for fast dummies to work correctly
data <- data %>% filter(!is.na(sex), !is.na(embarked), !is.na(deck))

#Create dummy variables
data <- data %>%
  fastDummies :: dummy_cols(select_columns = c("sex", "embarked","deck"),
                            remove_selected_columns = TRUE,
                            remove_first_dummy = TRUE)

#Remove NAs
data <-na.omit(data)
#head(data)
set.seed(15)

#split data into train and test
datasplit <- data %>%
  group_by(survived) %>%
  group_split() %>%
  lapply(function(df) sample(seq_len(nrow(df)), size = 0.7 * nrow(df))) %>%
  unlist()

data_train<-data[datasplit,]
data_test<-data[-datasplit,]

#check the data
cat("Training data set size:", nrow(data_train), "\n")
cat("Testing data set size:", nrow(data_test), "\n")
cat("Size of entire data set:", nrow(data), "\n")
head(data)






>>>>>>> b6ee2a8b254604a6b367704901081436bd711d3a
```
``` {r poisson}
  library(MASS)

<<<<<<< HEAD
```{r}
library(ggplot2)
################################################################################
#                  Function to plot missing data in DF                         #
################################################################################

plot_missing_barchart <- function(df) {
  # Calculate % of NA or empty strings per column
  na_empty_pct <- sapply(df, function(col) {
    mean(is.na(col) | col == "")
  }) * 100
  
  # Create a dataframe from the result
  na_df <- data.frame(
    Column = names(na_empty_pct),
    PercentMissing = na_empty_pct
  )
  
  # Plot the bar chart
  ggplot(na_df, aes(x = reorder(Column, -PercentMissing), y = PercentMissing)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    coord_flip() +
    labs(title = "Percentage of NA or Empty Cells per Column",
         x = "Column",
         y = "Percentage (%)") +
    theme_minimal()
}
```


```{r}
################################################################################
#                           Load Data                                          #
################################################################################
odata <- read.csv("data/Titanic_Survival_Data.csv")
cat("Size of entire data set:", nrow(odata), "\n")
```

```{r}

################################################################################
#                           Remove Un-needed Cols                              # 
################################################################################
# Name: Removing because names have no inference on surivival (inference)
# ticket: Ticket No. will also likely not have an influence in survival
# boat: This is highly correlated to the survival dependant variable since people
#       who made it on a boat likely survived
# body: This is highly correlated to the survival dependant variable since people
#       who's body was recovered did not survive.
# home.dest: The destination likely has nothing to do with the survival

data.clean = odata[, !(names(odata) %in% c("name", "ticket", "boat","body","home.dest"))]

################################################################################
#                           Data Augmentation                                  #   
################################################################################
#Extract deck letter from cabin
data.clean$deck <- substr(data.clean$cabin, 1,1)
# Remove cabin col:
data.clean$cabin <- NULL

################################################################################
#                           Check for Missing values                           #
################################################################################
# Function to calculate % of NA or empty strings per column

plot_missing_barchart(data.clean)
################################################################################
#                           Imputing data                                      #   
################################################################################

# ---- Age----
#Replace NAs in age column with Median value 
median_age <- median(data.clean$age, na.rm = TRUE)
data.clean <- data.clean %>%
  mutate(age = ifelse(is.na(age), median_age, age))

# ---- deck----
# For deck, since its a category, we decided to use KNN  to impute the column:

# Install if not already installed
# install.packages("VIM")
library(VIM)

# Replace "" with NA in the 'deck' column
data.clean$deck[data.clean$deck == ""] <- NA

# Convert 'cabin' to factor
data.clean$deck <- as.factor(data.clean$deck)

# Apply kNN imputation just to Cabin column
data.clean <- kNN(data.clean, variable = "deck", k = 5)

# Check that NAs were imputed
# sum(is.na(data.clean$deck))        # Original
# sum(is.na(data.clean.imputed$deck)) # After

# Remove indicator col:
data.clean$deck_imp <- NULL
################################################################################
#          Check for Missing values after Imputation                           #
################################################################################
# Function to calculate % of NA or empty strings per column
plot_missing_barchart(data.clean)

################################################################################
#                           Check categorical cols                             # 
################################################################################

for (colname in names(data.clean)) {
  # Count unique categories (including NA if present)
  unique_vals <- table(odata[[colname]], useNA = "ifany")
  
  # Only print columns with 5 or fewer categories
  if (length(unique_vals) <= 10) {
    cat("----", colname, "----\n")
    print(unique_vals)
    cat("\n\n")
  }
}
table(data.clean$deck)
################################################################################
#                           Dummify Cat. cols                                  # 
################################################################################
# Dummifying pclass:
data.clean$pclass_1 = ifelse(data.clean$pclass == 1, 1, 0)
data.clean$pclass_2 = ifelse(data.clean$pclass == 2, 1, 0)

# Dummifying sex:
data.clean$sex_M = ifelse(data.clean$sex == 'male', 1, 0)

# Dummifying embarked:
data.clean$embarked_C = ifelse(data.clean$embarked == 'C', 1, 0)
data.clean$embarked_Q = ifelse(data.clean$embarked == 'Q', 1, 0)

# Dummifying deck:
data.clean$deck_A = ifelse(data.clean$deck == 'A', 1, 0)
data.clean$deck_B = ifelse(data.clean$deck == 'B', 1, 0)
data.clean$deck_C = ifelse(data.clean$deck == 'C', 1, 0)
data.clean$deck_D = ifelse(data.clean$deck == 'D', 1, 0)
data.clean$deck_E = ifelse(data.clean$deck == 'E', 1, 0)
data.clean$deck_F = ifelse(data.clean$deck == 'F', 1, 0)
data.clean$deck_G = ifelse(data.clean$deck == 'G', 1, 0)

# Dummifying sibsp:
data.clean$sibsp_1 = ifelse(data.clean$sibsp == 1, 1, 0)
data.clean$sibsp_2 = ifelse(data.clean$sibsp == 2, 1, 0)
data.clean$sibsp_3 = ifelse(data.clean$sibsp == 3, 1, 0)
data.clean$sibsp_4 = ifelse(data.clean$sibsp == 4, 1, 0)
data.clean$sibsp_5 = ifelse(data.clean$sibsp == 5, 1, 0)
data.clean$sibsp_8 = ifelse(data.clean$sibsp == 8, 1, 0)

# Dummifying parch:
data.clean$parch_1 = ifelse(data.clean$parch == 1, 1, 0)
data.clean$parch_2 = ifelse(data.clean$parch == 2, 1, 0)
data.clean$parch_3 = ifelse(data.clean$parch == 3, 1, 0)
data.clean$parch_4 = ifelse(data.clean$parch == 4, 1, 0)
data.clean$parch_5 = ifelse(data.clean$parch == 5, 1, 0)
data.clean$parch_6 = ifelse(data.clean$parch == 6, 1, 0)
data.clean$parch_9 = ifelse(data.clean$parch == 9, 1, 0)

# Removing Dummified cols:
data.clean = subset(data.clean, select  = -c(pclass, sex, embarked,deck, sibsp, 
                                             parch))

```
```{r}
################################################################################
#                           Remove NA rows                                     # 
################################################################################
data.clean = na.omit(data.clean)

cat(nrow(odata) - nrow(data.clean),'rows were removed from original dataset')

################################################################################
#          Check for Missing values after na.omit()                            #
################################################################################
# Function to calculate % of NA or empty strings per column
plot_missing_barchart(data.clean)

################################################################################
#                           Divide in Test/Train                               # 
################################################################################

# Diving the data into Training and Test
# Sets the same seed for the random sampling after, usefull for reproducability. 
set.seed(567)
# NOTE: sample() expects a vector as the first kwarg, so you can't pass it a
# dataframe directly, so instead you feed it a vector with the number of rows 
# of the DF (i.e. 1 : nrow(crime_rate)), and use the results as indices to filter
# your original DF later on.
# If you want a more direct way to do this, you can use sample_n() from
# library(dplyr), this one does take a DF as input. 
train_indices = sample(1 : nrow(data.clean), size = 0.70*nrow(data.clean), replace = FALSE)
train = data.clean[train_indices,]
test = data.clean[-train_indices,]
cat("We are using:", nrow(train)/nrow(data.clean) * 100, '% of the data for training')
```


```{r}
################################################################################
#                           Legacy Code                                        # 
################################################################################

#Remove Unneeded Columns'
# data <- odata[, !(names(odata) %in% c("name", "ticket", "boat","body","home.dest"))]
# 
# #Replace NAs in age column with Median value 
# median_age <- median(data$age, na.rm = TRUE)
# data <- data %>%
#   mutate(age = ifelse(is.na(age), median_age, age))
# 
# #Extract deck letter from cabin
# data$deck <- substr(data$cabin, 1,1)
# data$cabin <- NULL
# 
# #Create dummy variables
# data <- data %>%
#   fastDummies :: dummy_cols(select_columns = c("sex", "embarked","deck"),
#                             remove_selected_columns = TRUE)
# 
# # Remove NAs
# data <-na.omit(data)
# #head(data)
# set.seed(15)
# 
# #split data into train and test
# datasplit <- data %>%
#   group_by(survived) %>%
#   group_split() %>%
#   lapply(function(df) sample(seq_len(nrow(df)), size = 0.7 * nrow(df))) %>%
#   unlist()
# 
# data_train<-data[datasplit,]
# data_test<-data[-datasplit,]
# 
# #check the data
# cat("Training data set size:", nrow(data_train), "\n")
# cat("Testing data set size:", nrow(data_test), "\n")
# cat("Size of entire data set:", nrow(data), "\n")
# head(data)
```
``` {r poisson}
  library(MASS)

  # Returns the result of different p cutoff values for 0 <= p <= 1 in 0.01 increments
  cutoff.prg<-function(pred,act){
    p<-seq(0,1,0.01)
    n<-length(p)
    out<-matrix(0,nrow=n,ncol=12)
    for(i in 1:n){
      predictions <- ifelse(pred >p[i], 1, 0)
      confusion_matrix <- confusionMatrix(as.factor(predictions),as.factor(act),mode="prec_recall", positive = "1")
      out[i,]<-cbind(p=p[i],t(confusion_matrix[[4]]))
    }
    dimnames(out)[[2]]<-c("p","Sensitivity","Specificity","Pos Pred Value","Neg Pred Value","Precision","Recall", "F1","Prevalence","Detection Rate","Detection Prevalence", "Balanced Accuracy")
    out
  }

  poissonReg <- glm(survived~.,family=poisson, train)
  summary(poissonReg)
=======
  cutoff.prg<-function(pred,act){
  # pred<-predicted_probabilities
  # act<-true_labels
  p<-seq(0,1,0.01)
  n<-length(p)
  out<-matrix(0,nrow=n,ncol=12)
  for(i in 1:n){
  predictions <- ifelse(pred >p[i], 1, 0)
  confusion_matrix <- confusionMatrix(as.factor(predictions),as.factor(act),mode="prec_recall", positive = "1")
  out[i,]<-cbind(p=p[i],t(confusion_matrix[[4]]))
  }
  dimnames(out)[[2]]<-c("p","Sensitivity","Specificity","Pos Pred Value","Neg Pred Value","Precision","Recall", "F1","Prevalence","Detection Rate","Detection Prevalence", "Balanced Accuracy")
  out
  }

  poissonReg <- glm(survived~.,family=poisson, data_train)
  sumary(poissonReg)
>>>>>>> b6ee2a8b254604a6b367704901081436bd711d3a
  
  # compare the fitted model to the null model
  1-pchisq(24.41141, 12)
  
  anova(poissonReg,test="Chi")
  drop1(poissonReg,test="Chi")
  
  # Testing against test data
<<<<<<< HEAD
  predicted_probs <- predict(poissonReg, newdata = test, type = "response")
  
  predictions <- ifelse(predicted_probs > 0.49, 1, 0)
  confusion_matrix <- confusionMatrix(as.factor(predictions), as.factor(test$survived), mode="prec_recall", positive = "1")
  confusion_matrix 
  
  # Get the results of different cutoff values
  testResult<-cutoff.prg(predicted_probs,test$survived)
  
  # We get get the optimal p cut off value based on the maximum F1 score
  plot(testResult[,1],testResult[,8],xlab="cut off",ylab="F1")
  testResult[which.max(testResult[,8]),]
=======
  
  predicted_probs <- predict(poissonReg, newdata = data_test, type = "response")
  
  predictions <- ifelse(predicted_probs > 0.49, 1, 0)
  confusion_matrix <- confusionMatrix(as.factor(predictions), as.factor(data_test$survived), mode="prec_recall", positive = "1")
  confusion_matrix 
  
  test<-cutoff.prg(predicted_probs,data_test$survived)
  plot(test[,1],test[,8],xlab="cut off",ylab="F1")
  test[which.max(test[,8]),]
>>>>>>> b6ee2a8b254604a6b367704901081436bd711d3a

```
\par Ho: Variables are not important
\par Ha: Variable are important
\par Since we have a p-value of 0.017, we reject the null hypothesis. Variables are important.

\par According to the regression the male variable decreases the log-odds of survival by 0.78211, which translates to a 54% lower odds of survival compared to being female.
\par Assuming a alpha of 0.05, when adding sequentially to the model, only the sex_male variable is significant (a p-value of 1.368e-05). Additionally, when performing the drop one test, we see that only the sex_male variable is significantly affecting the model (a p-value of 2.575e-05). Therefore, the sex vairable is the only statistically significant variable within the model.

\par For the Poisson Regression, only sex_male is significant. 

<<<<<<< HEAD
\par Our Poisson Regression has a balanced accuracy of 81.28% with optimal cutoﬀ(.26) based on max F1 score (.7619).
=======
\par Our Poisson Regression has a balanced accuracy of 80.92% with optimal cutoﬀ(.49) based on max F1 score (.8302).
>>>>>>> b6ee2a8b254604a6b367704901081436bd711d3a
